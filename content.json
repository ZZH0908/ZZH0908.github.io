{"meta":{"title":"稍安勿躁zzh","subtitle":"Record Life","description":"最怕一生碌碌无为，还说平凡难能可贵","author":"zzh","url":"https://www.ahbzzzh.cn"},"pages":[{},{},{}],"posts":[{"title":"ML学习","date":"2019-02-01T14:49:50.000Z","path":"2019/02/01/ML学习/","content":"<div class=\"note primary\">\n            <p>机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科.专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能.它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎.——————百度百科 </p>\n          </div>\n<div class=\"note success\">\n            <p>准备参考《统计学习方法李航》、《机器学习实战》、《机器学习周志华》 </p>\n          </div>\n<ol>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/K%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/kNN.ipynb\" target=\"_blank\" rel=\"noopener\">k-Nearest Neighbor</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kMeans.ipynb\" target=\"_blank\" rel=\"noopener\">k均值聚类算法</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/Logistic/LogRegres.ipynb\" target=\"_blank\" rel=\"noopener\">logistic</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/regression_.ipynb\" target=\"_blank\" rel=\"noopener\">线性回归</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/D_Tree/d_tree.ipynb\" target=\"_blank\" rel=\"noopener\">决策树</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/bayes/bayes.ipynb\" target=\"_blank\" rel=\"noopener\">朴素贝叶斯</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/Adaboost/Adaboost_.ipynb\" target=\"_blank\" rel=\"noopener\">Adaboost元算法</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/SVM/svm.ipynb\" target=\"_blank\" rel=\"noopener\">svm支持向量机</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/pca/pca.ipynb\" target=\"_blank\" rel=\"noopener\">pca</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/SVD/SVD.ipynb\" target=\"_blank\" rel=\"noopener\">SVD</a></li>\n</ol>\n<p><a href=\"https://github.com/ZZH0908/Learning-notes/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98\" target=\"_blank\" rel=\"noopener\">我的git</a></p>\n","permalink":"https://www.ahbzzzh.cn/2019/02/01/ML学习/","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.ahbzzzh.cn/categories/机器学习/"}],"tags":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://www.ahbzzzh.cn/tags/Machine-learning/"},{"name":"python","slug":"python","permalink":"https://www.ahbzzzh.cn/tags/python/"}]},{"title":"matplotlib入门","date":"2019-01-29T15:28:57.000Z","path":"2019/01/29/matplotlib入门/","content":"<div class=\"note success\">\n            <p>Matplotlib是一个Python 2D绘图库，Matplotlib试图让简单的事情变得简单让困难的变为可能.只需几行代码即可生成绘图，直方图，功率谱，条形图，错误图，散点图等.可用于Python脚本，Python和IPython shell，Jupyter笔记本，Web应用程序服务器和四个图形用户界面工具包 </p>\n          </div>\n<blockquote>\n<p>这篇博文参考了matplotlib for python官方教程，根据个人用途对教程进行筛选学习，因为全部都看太浪费时间，并且matplotlib库对一般使用者来说不需要详细了解其源代码，熟练掌握使用就足够了</p>\n</blockquote>\n<p>以下贴出学习记录的链接：</p>\n<ol>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.ipynb\" target=\"_blank\" rel=\"noopener\">使用指南</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/Pyplot%20tutorial.ipynb\" target=\"_blank\" rel=\"noopener\">Pyplot tutorial</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/Image%20tutorial.ipynb\" target=\"_blank\" rel=\"noopener\">Image tutorial</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/Legend%20.ipynb\" target=\"_blank\" rel=\"noopener\">Legend</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9B%BE%E5%B8%83%E5%B1%80.ipynb\" target=\"_blank\" rel=\"noopener\">自定义布局</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/Constrained%20Layout%20Guide.ipynb\" target=\"_blank\" rel=\"noopener\">Constrained Layout Guide</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/Tight%20Layout%20guide.ipynb\" target=\"_blank\" rel=\"noopener\">Tight Layout guide</a></li>\n<li><a href=\"https://nbviewer.jupyter.org/github/ZZH0908/Learning-notes/blob/master/matplotlib/Text%20in%20Matplotlib%20Plots.ipynb\" target=\"_blank\" rel=\"noopener\">Text in Matplotlib Plots</a></li>\n</ol>\n<p>后续还会对教程进行温习修正，会逐渐加入更多的官方教程内容以及自己的用到的实例，拜拜~<br>参考：</p>\n<ul>\n<li><a href=\"https://matplotlib.org/tutorials/index.html#introductory\" target=\"_blank\" rel=\"noopener\">https://matplotlib.org/tutorials/index.html#introductory</a></li>\n</ul>\n","permalink":"https://www.ahbzzzh.cn/2019/01/29/matplotlib入门/","categories":[{"name":"python","slug":"python","permalink":"https://www.ahbzzzh.cn/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.ahbzzzh.cn/tags/python/"},{"name":"可视化","slug":"可视化","permalink":"https://www.ahbzzzh.cn/tags/可视化/"}]},{"title":"C语言记录四","date":"2019-01-22T10:24:10.000Z","path":"2019/01/22/C语言记录四/","content":"<h2 id=\"存储类、链接和内存管理\"><a href=\"#存储类、链接和内存管理\" class=\"headerlink\" title=\"存储类、链接和内存管理\"></a>存储类、链接和内存管理</h2><p><img src=\"https://i.imgur.com/tbgtmHa.png\" alt=\"\"></p>\n<h2 id=\"文件输入输出\"><a href=\"#文件输入输出\" class=\"headerlink\" title=\"文件输入输出\"></a>文件输入输出</h2>","permalink":"https://www.ahbzzzh.cn/2019/01/22/C语言记录四/","categories":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/tags/C语言/"}]},{"title":"Faster-RCNN","date":"2019-01-11T15:52:08.000Z","path":"2019/01/11/Faster-RCNN/","content":"<blockquote>\n<p>候选区域计算一直是目标检测算法的瓶颈，FasterRCNN提出RPN网络，能够在共享卷积特征上预测region proposals，并且只付出微小的计算代价，结构上利用卷积操作代替全连接层添加额外的层同时预测每个位置的目标边界和目标置信度.RPN网络是端到端的网络，能够产生少量(仅仅300个)但高质量的region proposals，进一步将RPN和Fast RCNN结合为共享卷积层特征的单个网络.</p>\n</blockquote>\n<h2 id=\"FASTER-R-CNN\"><a href=\"#FASTER-R-CNN\" class=\"headerlink\" title=\"FASTER R-CNN\"></a>FASTER R-CNN</h2><p>整体结构<br><img src=\"https://i.imgur.com/34pK7vB.png\" alt=\"\"></p>\n<h3 id=\"Region-Proposal-Networks\"><a href=\"#Region-Proposal-Networks\" class=\"headerlink\" title=\"Region Proposal Networks\"></a>Region Proposal Networks</h3><p>RPN网络是基于最后一层卷积特征的也就是共享特征，这里不做介绍直接贴出RPN网络的关键结构图.<br><img src=\"https://i.imgur.com/BREyQXY.png\" alt=\"\"><br>&#160; &#160; &#160; &#160;如图所示，RPN网络也是以共享的卷积特征为基础，利用滑窗的方式(3×3卷积)随后紧跟着单位卷积得到两个分支输出；这里还要说一下anchor的概念，每一个anchor都以滑动窗口为中心，anchor是借鉴了金字塔的思想，默认使用三种尺度和三种比例一共可以得到9种不同的.代码示例：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rpn</span><span class=\"params\">(base_layers, num_anchors)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    x = Conv2D(<span class=\"number\">512</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), padding=<span class=\"string\">'same'</span>, activation=<span class=\"string\">'relu'</span>, kernel_initializer=<span class=\"string\">'normal'</span>, name=<span class=\"string\">'rpn_conv1'</span>)(base_layers)</span><br><span class=\"line\"></span><br><span class=\"line\">    x_class = Conv2D(num_anchors, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'sigmoid'</span>, kernel_initializer=<span class=\"string\">'uniform'</span>, name=<span class=\"string\">'rpn_out_class'</span>)(x)</span><br><span class=\"line\">    x_regr = Conv2D(num_anchors * <span class=\"number\">4</span>, (<span class=\"number\">1</span>, <span class=\"number\">1</span>), activation=<span class=\"string\">'linear'</span>, kernel_initializer=<span class=\"string\">'zero'</span>, name=<span class=\"string\">'rpn_out_regress'</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [x_class, x_regr, base_layers]</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>anchors的思想方法使得网络具有translation invariant并使用很少的参数，也使得网络不需要多尺度输入和多尺度滤波器就能很好的学习并输出高质量的候选区.</p>\n</blockquote>\n<h3 id=\"Loss-Function\"><a href=\"#Loss-Function\" class=\"headerlink\" title=\"Loss Function\"></a>Loss Function</h3><p>公式：</p>\n<script type=\"math/tex; mode=display\">L\\left ( \\left \\{ p_{i} \\right \\},\\left \\{ t_{i} \\right \\} \\right )=\\frac{1}{N_{cls}}\\underset{i}{\\sum }L_{cls}\\left ( p_{i},p_{i}^{\\star } \\right )+\\lambda \\frac{1}{N_{reg}}\\underset{i}{\\sum }p_{i}^{\\star }L_{reg}\\left ( t_{i},t_{i}^{\\star } \\right )</script><p>第一项预测anchor是否为一个目标，第二项则是smooth L1函数用于回归.</p>\n<h2 id=\"Sharing-Features-for-RPN-and-Fast-R-CNN\"><a href=\"#Sharing-Features-for-RPN-and-Fast-R-CNN\" class=\"headerlink\" title=\"Sharing Features for RPN and Fast R-CNN\"></a>Sharing Features for RPN and Fast R-CNN</h2><p>学习一个统一网络</p>\n<ol>\n<li><p>Alternating training<br>首先训练RPN网络，然后利用RPN的输出proposals来训练Fast RCNN；再用，Fast RCNN网络来初始化RPN，一次迭代.</p>\n</li>\n<li><p>Approximate joint training<br>两个网络被合并为一个网络进行训练，前向计算产生的region proposals被视为固定的来训练Fast RCNN网络；BP算法和平常一样，RPN loss和Fast RCNN loss组合进行优化；但这忽视了proposals boxe位置的导数；反正我没读懂~~~~</p>\n</li>\n<li><p>Non-approximate joint training<br>需要RoI池化层对区域建议可微，需要RoI变形层实现，作者也没详细说明.最终采样了第一种训练方法，通过4步训练算法，交替优化学习至共享特征：第一步，先对RPN网络进行训练(使用预训练网络然后进行端到端的微调)；第二步，利用第一步RPN产生的proposals训练Fast R-CNN；第三步，用Fast R-CNN来初始化RPN，固定卷积层，仅仅微调RPN特殊的层；第四步，此时已经共享了卷积层，再微调Fast R-CNN的特殊层.</p>\n</li>\n</ol>\n<p>&#160; &#160; &#160; &#160;这里说下anchor数量，对于一张1000×600的图像，在得到的特征图中会有大约20000个anchor，忽略掉超出边界的，还剩大约6000个，在经过NMS(阈值为0.7)非最大抑制处理后还剩大约2000个anchor.</p>\n","permalink":"https://www.ahbzzzh.cn/2019/01/11/Faster-RCNN/","categories":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/categories/目标检测/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/tags/目标检测/"},{"name":"论文","slug":"论文","permalink":"https://www.ahbzzzh.cn/tags/论文/"}]},{"title":"FastRCNN","date":"2019-01-10T01:36:21.000Z","path":"2019/01/10/FastRCNN/","content":"<div class=\"note success\">\n            <p>Fast R-CNN采用了一些创新来提高训练和测试的速度，同时也提高了检测的准确率.将目标分类和定位联合学习，这种单步训练方式加快了训练速度. </p>\n          </div>\n<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p><strong>目标探测算法有两个主要挑战：</strong><br>1.图像需要进行预处理来提取大量的候选区；<br>2.这些目标候选区域仅仅是粗糙的定位，需要进行重新调整才能实现更精确的定位。而简化算法、算法效果、算法速度恰恰是相互抵触的！</p>\n<blockquote>\n<p>RCNN、SPPnet都需要多阶段训练微调、占用空间大、训练和测试速度慢等缺点，SPPnet还无法更新spp层之前卷积层的参数，对模型有所限制，而Fast RCNN就解决了以上问题<br><strong>本文的主要贡献</strong><br><img src=\"https://i.imgur.com/498LEga.png\" alt=\"\"></p>\n</blockquote>\n<h2 id=\"FastR-CNNarchitectureandtraining\"><a href=\"#FastR-CNNarchitectureandtraining\" class=\"headerlink\" title=\"FastR-CNNarchitectureandtraining\"></a>FastR-CNNarchitectureandtraining</h2><p>整体结构图<br><img src=\"https://i.imgur.com/r0nxtk2.png\" alt=\"\"><br>&#160; &#160; &#160; &#160;先对整张图片进行特征提取，然后对某个特征图上的候选框特征进行ROI层操作处理成固定长度的向量，然后接入全连接层，最后全连接层输入分为两个分支：一个是类别概率分布(包括背景类别)，另一个对每个类别都产生四个预测值，进行回归调整，对应bbox信息.</p>\n<h3 id=\"TheRoIpoolinglayer\"><a href=\"#TheRoIpoolinglayer\" class=\"headerlink\" title=\"TheRoIpoolinglayer\"></a>TheRoIpoolinglayer</h3><p>&#160; &#160; &#160; &#160;和SPPnet的空间金字塔池化操作一样，只不过这里只有一个level</p>\n<h3 id=\"较高的训练效率\"><a href=\"#较高的训练效率\" class=\"headerlink\" title=\"较高的训练效率\"></a>较高的训练效率</h3><p>&#160; &#160; &#160; &#160;SPPnet有一个缺点是无法更新或者说无法有效的对SPP之前的卷积层参数，分析原因：ROI所对应的感受野都有一个很大的感受野，占据整个图像大部分区域，再者，每个ROI采样自不同的图像样本，这样不能共享计算；而FastRCNN的采用模式为先对图像进行采样，再对这些图像采样ROI，所以每个minibatch都是固定的几张图像，可以共享计算和内存.</p>\n<h3 id=\"Multi-task-loss\"><a href=\"#Multi-task-loss\" class=\"headerlink\" title=\"Multi-task loss.\"></a>Multi-task loss.</h3><p>&#160; &#160; &#160; &#160;多任务损失，第一个分支输出为ROI的类别概率分布<script type=\"math/tex\">p=\\left ( p_{0},....,p_{K} \\right )</script>，另一个输出分支为bbox回归的offsets偏移量<script type=\"math/tex\">t^{k}=\\left \\{ t_{x}^{k},t_{y}^{k},t_{w}^{k},t_{y}^{k} \\right \\}</script>，联合损失函数L如下:</p>\n<script type=\"math/tex; mode=display\">L\\left ( p,u,t^{u},v \\right )=L_{cls}\\left ( p,u \\right )+\\lambda [u\\geqslant 1]L_{loc}\\left ( t^{u},v \\right )</script><p>其中，[]是一个指示函数，用于控制只有目标才进行位置回归，背景类别就不需要进行位置回归。λ是超参数，用于控制均衡.u、v为标签(回归目标v是被处理成0均值和单位方差).回归loss公式为：</p>\n<script type=\"math/tex; mode=display\">L_{loc}\\left ( t^{u},v \\right )=\\underset{i\\in \\left \\{ x,y,w,h \\right \\} }{\\sum }smooth_{L_{1}}\\left ( t_{i}^{u}-v_{i} \\right )</script><p>进一步：</p>\n<script type=\"math/tex; mode=display\">smooth_{L_{1}}\\left ( x \\right )=\\left\\{\\begin{matrix}\n&0.5x^{2}  &if|x|< 1 \\\\ \n&|x|-0.5  &otherwise \n\\end{matrix}\\right.</script><p>&#160; &#160; &#160; &#160;与L2 loss相比L1对异常值更鲁棒是因为，当预测值和真实值相差较大时，可以防止梯度爆炸，画图即可得知<br><img src=\"https://i.imgur.com/lqB6Zwk.png\" alt=\"\"></p>\n<h3 id=\"RoI池化层的反向传播\"><a href=\"#RoI池化层的反向传播\" class=\"headerlink\" title=\"RoI池化层的反向传播\"></a>RoI池化层的反向传播</h3><p>RoI pooling层的反向传播计算公式：<br><img src=\"https://i.imgur.com/AwhqZ8v.png\" alt=\"\"><br>&#160; &#160; &#160; &#160;激活单元会出现重叠像素的情况即ROI重叠，只对最大值进行求偏导，并且偏导是被累积的.还有超参数设置和Scale invariance、Mini-batch sampling等不作详述.</p>\n<h2 id=\"detection\"><a href=\"#detection\" class=\"headerlink\" title=\"detection\"></a>detection</h2><p>&#160; &#160; &#160; &#160;网络与一张图像和一些候选对象区域(selective search)作为输入，对每个ROI产生两个分支输出，还设置了一个概率估计来衡量其为一个类别的置信度，然后进行NMS处理.</p>\n<h3 id=\"Truncated-SVD-for-faster-detection\"><a href=\"#Truncated-SVD-for-faster-detection\" class=\"headerlink\" title=\"Truncated SVD for faster detection\"></a>Truncated SVD for faster detection</h3><p>&#160; &#160; &#160; &#160;对于普通的分类网络，全连接层的计算用时不及卷积层的计算，但是针对object detection，Fast RCNN在ROI pooling后每个region proposal(数量太多)都要经过几个全连接层，使得全连接层计算耗时太长，如下图，所以作者采用SVD来简化全连接层的计算，SVD在图像压缩上有应用.<br><img src=\"https://i.imgur.com/F1LAaHy.png\" alt=\"\"></p>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ul>\n<li><a href=\"https://arxiv.org/abs/1504.08083\" target=\"_blank\" rel=\"noopener\">Fast R-CNN</a></li>\n</ul>\n","permalink":"https://www.ahbzzzh.cn/2019/01/10/FastRCNN/","categories":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/categories/目标检测/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/tags/目标检测/"},{"name":"论文","slug":"论文","permalink":"https://www.ahbzzzh.cn/tags/论文/"}]},{"title":"SPPnet","date":"2019-01-08T13:48:00.000Z","path":"2019/01/08/SPPnet/","content":"<h2 id=\"SPP-net解决的问题\"><a href=\"#SPP-net解决的问题\" class=\"headerlink\" title=\"SPP-net解决的问题\"></a>SPP-net解决的问题</h2><p><strong>贡献：</strong><br>1.解决了CNN网络要求固定尺寸输入的问题，固定尺寸通常会导致目标物体变形不利于识别.<br>解决方式：使用空间金字塔池化的方法，使得网络可以对任意尺寸比例的图像都能产生固定尺寸的表示.<br>2.使网络处理速度更快.<br>解决方式：对图像特征只提取一次，然后池化这些特征以产生固定的表示用于探测器训练，与RCNN相比不用重复的计算前面的特征，共享特征使得SPPnet处理速度更快.结合空间金字塔池化方法也实现了更高的acc，数据集为Pascal voc 2007.<br><strong>Spatial pyramid pooling的优点：</strong><br>1.可以产生固定的输出表示，可以忽略输入尺寸<br>2.multi-level spatial bins可以进行多级池化，使得网络对目标形变更加鲁棒<br>3.由于多样尺寸的输入，可以提取到更多尺度的特征<br><img src=\"https://i.imgur.com/I8vtYj8.png\" alt=\"\"></p>\n<h2 id=\"spatial-pyramid-pooling-layer原理图\"><a href=\"#spatial-pyramid-pooling-layer原理图\" class=\"headerlink\" title=\"spatial pyramid pooling layer原理图\"></a>spatial pyramid pooling layer原理图</h2><p><img src=\"https://i.imgur.com/Or0XjKR.png\" alt=\"\"></p>\n<h2 id=\"SPP-NET-FOR-OBJECT-DETECTION\"><a href=\"#SPP-NET-FOR-OBJECT-DETECTION\" class=\"headerlink\" title=\"SPP-NET FOR OBJECT DETECTION\"></a>SPP-NET FOR OBJECT DETECTION</h2><p>和RCNN不同的是，SPPnet是对整个图像进行特征提取一次，这部分的计算和参数是共享的，原理图如下：<br><img src=\"https://i.imgur.com/3F5w8jG.png\" alt=\"\"></p>\n<blockquote>\n<p>算法流程：<br>对每张图像使用selective search 产生约2000个候选窗——-&gt;对整张图像进行特征提取<br>——-&gt;对特征图上对应的候选窗采用多级空间金字塔池化操作产生特征向量——-&gt;全连接层降维<br>——-&gt;二元线性SVM分类器训练</p>\n</blockquote>\n<p>随后作者做了standard hard negative mining、NMS、多尺度、微调、boundingboxregression等操作.<br>最后一张总结图：<br><img src=\"https://i.imgur.com/mQBtMpk.png\" alt=\"\"></p>\n","permalink":"https://www.ahbzzzh.cn/2019/01/08/SPPnet/","categories":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/categories/目标检测/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/tags/目标检测/"},{"name":"论文","slug":"论文","permalink":"https://www.ahbzzzh.cn/tags/论文/"}]},{"title":"OverFeat","date":"2019-01-03T06:16:10.000Z","path":"2019/01/03/OverFeat/","content":"<p><img src=\"https://i.imgur.com/ETYex2l.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<h2 id=\"论文的贡献\"><a href=\"#论文的贡献\" class=\"headerlink\" title=\"论文的贡献\"></a>论文的贡献</h2><ul>\n<li>单个共享网络可以执行多种任务</li>\n<li>在网络中使用了多尺度和滑动窗口的方法</li>\n<li>通过学习预测目标边界来进行定位</li>\n<li>首次解释了ConvNets是如何应用于定位和检测任务的</li>\n</ul>\n<p><strong>抛出问题：</strong></p>\n<blockquote>\n<p>以ImageNet举例，该数据集的大部分图像中，目标物体都粗略的出现在图像的中央并几乎充斥整个图像，但实际上在现实场景中是复杂的，目标物体出现的位置和大小是不确定的.</p>\n</blockquote>\n<p><strong>解决问题：</strong><br>1.传统方法</p>\n<ul>\n<li>在图像上多个位置都应用CNN提取特征，以滑窗的方式并且在多个尺度上应用；即便这样仍然会导致分类的效果良好，但在定位和目标检测任务上表现不佳(滑窗不一定能把一个物体完全地恰好的包含，有时候只包含目标的一部分)</li>\n</ul>\n<p>2.作者的思路</p>\n<ul>\n<li>设计训练一个网络，该网络对每一个窗口不仅仅只输出类别的概率分布，还要产生一个相对于窗口的目标定位预测以及bbox信息</li>\n<li>求出每个窗口每个类别的位置和置信度信息，简而言之就是剔除误差高的，留下最正确的</li>\n</ul>\n<h2 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h2><p><img src=\"https://i.imgur.com/ermwseR.png\" alt=\"模型结构\"></p>\n<h3 id=\"Multi-Scale-Classification\"><a href=\"#Multi-Scale-Classification\" class=\"headerlink\" title=\"Multi-Scale Classification\"></a>Multi-Scale Classification</h3><p>&#160; &#160; &#160; &#160;multi-view voting(四角和中心裁剪以及水平翻转)可以提高性能，但缺点是：1、忽略了图像的一部分区域.2、重叠部分的重复计算会浪费计算力.3、单一尺度不能使得网络获得最优的响应.<br>作者的思路是使用滑窗的方式应用于每个位置并且使用了多尺度的训练方法，滑窗和枚举类似在每个位置提取特征，但由于CNN的结构优越性，不会造成大量的计算量.而多尺度+multi-view voting会产生更多的views供网络学习，使得网络性能更稳定.<br>&#160; &#160; &#160; &#160;在网络下采样过程中，下采样比率越大信息损失的越多，粗糙的分布即过少的输出神经元不能使得网络窗口和目标对象很好的对齐，所以作者采用了一种分辨率增强的方法————对最后一个下采样操作中在不同的offsets上进行，弥补了分辨率的损失.如图所示，两个轴三种位移组合起来就是9种组合，这样操作能够得到更多的特征图而且是位置更多样的特征，这也是offsets起到的作用.<br>&#160; &#160; &#160; &#160;这些操作对水平翻转后的图像也要做一遍，最终产生的分类：(1)在每个尺度每个翻转版本获得每个类别的最大值.(2)取平均结果.(3)对类向量均值取top-1或top-2作为结果.<br><img src=\"https://i.imgur.com/Sf57Vkh.png\" alt=\"\"><br>结构大致分为三个部分，1-5层特征提取、全连接层起决策作用、offsets池化操作.offsets池化操作的目的是在更多的区域进行分类，这样目标物体才更大概率的被完整的进行分类.<br><img src=\"https://i.imgur.com/xfi1aPk.png\" alt=\"\"></p>\n<h2 id=\"Localization\"><a href=\"#Localization\" class=\"headerlink\" title=\"Localization\"></a>Localization</h2><p>使用训练过的分类模型，用regression<br>network代替分类器层进行训练，预测bbox，然后将二者结合.</p>\n<h3 id=\"Regressor-Training\"><a href=\"#Regressor-Training\" class=\"headerlink\" title=\"Regressor Training\"></a>Regressor Training</h3><p>为了能在尽可能多的区域预测，同样使用了offsets方法.<br><img src=\"https://i.imgur.com/YFtotWb.png\" alt=\"\"></p>\n<h3 id=\"Combining-Predictions\"><a href=\"#Combining-Predictions\" class=\"headerlink\" title=\"Combining Predictions\"></a>Combining Predictions</h3><p>筛选bbox<br><img src=\"https://i.imgur.com/Z9CuLUD.png\" alt=\"\"><br><img src=\"https://i.imgur.com/glSiuWR.png\" alt=\"\"></p>\n","permalink":"https://www.ahbzzzh.cn/2019/01/03/OverFeat/","categories":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/categories/目标检测/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/tags/目标检测/"},{"name":"论文","slug":"论文","permalink":"https://www.ahbzzzh.cn/tags/论文/"}]},{"title":"C语言记录三","date":"2018-12-27T07:16:49.000Z","path":"2018/12/27/C语言记录三/","content":"<p><img src=\"https://i.imgur.com/YZtERXW.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<h2 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h2><p><img src=\"https://i.imgur.com/tBwrAXV.png\" alt=\"\"></p>\n<h2 id=\"数组与指针\"><a href=\"#数组与指针\" class=\"headerlink\" title=\"数组与指针\"></a>数组与指针</h2><p><img src=\"https://i.imgur.com/d12TwOp.png\" alt=\"\"></p>\n<h2 id=\"字符串和字符串函数\"><a href=\"#字符串和字符串函数\" class=\"headerlink\" title=\"字符串和字符串函数\"></a>字符串和字符串函数</h2><p><img src=\"https://i.imgur.com/xApjYYU.png\" alt=\"\"></p>\n","permalink":"https://www.ahbzzzh.cn/2018/12/27/C语言记录三/","categories":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/tags/C语言/"}]},{"title":"C语言记录二","date":"2018-12-26T06:59:58.000Z","path":"2018/12/26/C语言记录二/","content":"<p><img src=\"https://i.imgur.com/iBD9ixR.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<h2 id=\"C控制语句：循环\"><a href=\"#C控制语句：循环\" class=\"headerlink\" title=\"C控制语句：循环\"></a>C控制语句：循环</h2><p><img src=\"https://i.imgur.com/C8s82eL.png\" alt=\"\"></p>\n<h2 id=\"C控制语句：分支和跳转\"><a href=\"#C控制语句：分支和跳转\" class=\"headerlink\" title=\"C控制语句：分支和跳转\"></a>C控制语句：分支和跳转</h2><p><img src=\"https://i.imgur.com/S4PCwET.png\" alt=\"\"></p>\n","permalink":"https://www.ahbzzzh.cn/2018/12/26/C语言记录二/","categories":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/tags/C语言/"}]},{"title":"C语言记录一","date":"2018-12-25T08:37:00.000Z","path":"2018/12/25/C语言记录一/","content":"<p><img src=\"https://i.imgur.com/TooNByc.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<h2 id=\"程序执行的两种方式\"><a href=\"#程序执行的两种方式\" class=\"headerlink\" title=\"程序执行的两种方式\"></a>程序执行的两种方式</h2><p><img src=\"https://i.imgur.com/bhWtt7v.png\" alt=\"\"></p>\n<h2 id=\"数据与C\"><a href=\"#数据与C\" class=\"headerlink\" title=\"数据与C\"></a>数据与C</h2><p><img src=\"https://i.imgur.com/8i8MKXR.png\" alt=\"\"></p>\n<h2 id=\"字符串与格式化输入-输出\"><a href=\"#字符串与格式化输入-输出\" class=\"headerlink\" title=\"字符串与格式化输入/输出\"></a>字符串与格式化输入/输出</h2><p><img src=\"https://i.imgur.com/AF1mwjR.png\" alt=\"\"></p>\n<h2 id=\"运算符、表达式和语句\"><a href=\"#运算符、表达式和语句\" class=\"headerlink\" title=\"运算符、表达式和语句\"></a>运算符、表达式和语句</h2><p><img src=\"https://i.imgur.com/yDlQQlE.png\" alt=\"\"></p>\n","permalink":"https://www.ahbzzzh.cn/2018/12/25/C语言记录一/","categories":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://www.ahbzzzh.cn/tags/C语言/"}]},{"title":"pytorch数据读取","date":"2018-11-24T12:16:46.000Z","path":"2018/11/24/pytorch数据读取/","content":"<p><img src=\"https://i.imgur.com/4zDrbvC.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>在pytorch官方入门文档分类任务的例子中，都使用了torchvision.datasets.ImageFolder API来读取数据，然后用torch.utils.data.DataLoader来进行迭代加载数据，这里只讨论前者<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">datasets</span>.<span class=\"title\">ImageFolder</span><span class=\"params\">(root, transform=None, target_transform=None, loader=&lt;function default_loader&gt;)</span></span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>root/dog/xxx.png<br>root/dog/xxy.png<br>root/dog/xxz.png</p>\n<p>root/cat/123.png<br>root/cat/nsdf3.png<br>root/cat/asd932_.png</p>\n</blockquote>\n<p>但数据文件夹目录必须遵循以上结构，在根目录下有文件名为类别名称的子文件夹这样的形式有点太固定了，最简单的解决办法可以把数据预处理成以上目录结构，但pytorch也提供了灵活的数据读取方式。<br>其实，torchvision.datasets.ImageFolder是Dataset的子类。先贴出Dataset代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Dataset</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"An abstract class representing a Dataset.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    All other datasets should subclass it. All subclasses should override</span></span><br><span class=\"line\"><span class=\"string\">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</span></span><br><span class=\"line\"><span class=\"string\">    supporting integer indexing in range from 0 to len(self) exclusive.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span><span class=\"params\">(self, index)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__add__</span><span class=\"params\">(self, other)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"自定义读取数据\"><a href=\"#自定义读取数据\" class=\"headerlink\" title=\"自定义读取数据\"></a>自定义读取数据</h2><p>由于猫狗数据集不是ImageFolder要求的形式，所以重写了代码：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">``Demo.py``</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Dataset</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"An abstract class representing a Dataset.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    All other datasets should subclass it. All subclasses should override</span></span><br><span class=\"line\"><span class=\"string\">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</span></span><br><span class=\"line\"><span class=\"string\">    supporting integer indexing in range from 0 to len(self) exclusive.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span><span class=\"params\">(self, index)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">raise</span> NotImplementedError</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__add__</span><span class=\"params\">(self, other)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ConcatDataset([self, other])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read_</span><span class=\"params\">(dir)</span>:</span></span><br><span class=\"line\">    class_to_idx = &#123;<span class=\"string\">'cat'</span>: <span class=\"number\">0</span>, <span class=\"string\">'dog'</span>: <span class=\"number\">1</span>&#125;</span><br><span class=\"line\">    images = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> file_name <span class=\"keyword\">in</span> os.listdir(dir):</span><br><span class=\"line\">        data_path = os.path.join(dir, file_name)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> file_name.split(<span class=\"string\">'.'</span>)[<span class=\"number\">0</span>] == <span class=\"string\">'cat'</span>:</span><br><span class=\"line\">            images.append((data_path, class_to_idx[<span class=\"string\">'cat'</span>]))</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            images.append((data_path, class_to_idx[<span class=\"string\">'dog'</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> images, class_to_idx</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pil_loader</span><span class=\"params\">(path)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(path, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        img = Image.open(f)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> img.convert(<span class=\"string\">'RGB'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">accimage_loader</span><span class=\"params\">(path)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> accimage</span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> accimage.Image(path)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> IOError:</span><br><span class=\"line\">        <span class=\"comment\"># Potentially a decoding problem, fall back to PIL.Image</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> pil_loader(path)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">default_loader</span><span class=\"params\">(path)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> get_image_backend</span><br><span class=\"line\">    <span class=\"keyword\">if</span> get_image_backend() == <span class=\"string\">'accimage'</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> accimage_loader(path)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> pil_loader(path)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">cat_dog</span><span class=\"params\">(Dataset)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, root, loader=default_loader, transform=None)</span>:</span></span><br><span class=\"line\">        samples, class_to_idx = read_(root)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(samples) == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span>(RuntimeError(<span class=\"string\">\"Found 0 files in subfolders of: \"</span> + root + <span class=\"string\">\"\\n\"</span></span><br><span class=\"line\">                               <span class=\"string\">\"Supported extensions are: \"</span> + <span class=\"string\">\",\"</span>.join(extensions)))</span><br><span class=\"line\">        self.samples = samples</span><br><span class=\"line\">        self.class_to_idx = class_to_idx</span><br><span class=\"line\">        self.root = root</span><br><span class=\"line\">        self.loader = loader</span><br><span class=\"line\">        self.transform = transform</span><br><span class=\"line\">        self.classes = [<span class=\"string\">'cat'</span>, <span class=\"string\">'dog'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span><span class=\"params\">(self, index)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">        path, target = self.samples[index]</span><br><span class=\"line\">        sample = self.loader(path)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.transform <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">            sample = self.transform(sample)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> sample, target</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> len(self.samples)</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.optim <span class=\"keyword\">import</span> lr_scheduler</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> datasets, models</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> copy</span><br><span class=\"line\"><span class=\"keyword\">import</span> Demo</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"comment\"># plt.ion()   # interactive mode</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">imshow</span><span class=\"params\">(inp, title=None)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Imshow for Tensor.\"\"\"</span></span><br><span class=\"line\">    inp = inp.numpy().transpose((<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\">    mean = np.array([<span class=\"number\">0.485</span>, <span class=\"number\">0.456</span>, <span class=\"number\">0.406</span>])</span><br><span class=\"line\">    std = np.array([<span class=\"number\">0.229</span>, <span class=\"number\">0.224</span>, <span class=\"number\">0.225</span>])</span><br><span class=\"line\">    inp = std * inp + mean</span><br><span class=\"line\">    inp = np.clip(inp, <span class=\"number\">0</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.imshow(inp)</span><br><span class=\"line\">    <span class=\"comment\"># plt.show()</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> title <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        plt.title(title)</span><br><span class=\"line\">    plt.pause(<span class=\"number\">10</span>)  <span class=\"comment\"># pause a bit so that plots are updated</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    transfer = T.Compose([</span><br><span class=\"line\">                        <span class=\"comment\"># T.RandomResizedCrop((256, 256)),</span></span><br><span class=\"line\">                        T.Resize((<span class=\"number\">256</span>, <span class=\"number\">256</span>)),</span><br><span class=\"line\">                        T.ToTensor(),</span><br><span class=\"line\">                        T.Normalize([<span class=\"number\">0.485</span>, <span class=\"number\">0.456</span>, <span class=\"number\">0.406</span>], [<span class=\"number\">0.229</span>, <span class=\"number\">0.224</span>, <span class=\"number\">0.225</span>])</span><br><span class=\"line\">                        ])</span><br><span class=\"line\"></span><br><span class=\"line\">    image_datasets  = Demo.cat_dog(<span class=\"string\">'E:/database/cat_dog/train'</span>, transform=transfer)</span><br><span class=\"line\">    dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=<span class=\"number\">4</span>,</span><br><span class=\"line\">                                                 shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    class_names = image_datasets.classes</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Get a batch of training data</span></span><br><span class=\"line\">    inputs, classes = next(iter(dataloaders))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Make a grid from batch</span></span><br><span class=\"line\">    out = torchvision.utils.make_grid(inputs)</span><br><span class=\"line\"></span><br><span class=\"line\">    imshow(out, title=[class_names[x] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> classes])</span><br><span class=\"line\">    <span class=\"comment\"># imshow(inputs[0], title=[class_names[x] for x in classes])</span></span><br></pre></td></tr></table></figure>\n<p>显示<br><img src=\"https://i.imgur.com/lsnGyy1.png\" alt=\"\"></p>\n<h2 id=\"TensorDataset\"><a href=\"#TensorDataset\" class=\"headerlink\" title=\"TensorDataset\"></a>TensorDataset</h2><p>pytorch提供的另一种读取api，也是继承了Dataset类，源码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TensorDataset</span><span class=\"params\">(Dataset)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Dataset wrapping tensors.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Each sample will be retrieved by indexing tensors along the first dimension.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">        *tensors (Tensor): tensors that have the same size of the first dimension.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, *tensors)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> all(tensors[<span class=\"number\">0</span>].size(<span class=\"number\">0</span>) == tensor.size(<span class=\"number\">0</span>) <span class=\"keyword\">for</span> tensor <span class=\"keyword\">in</span> tensors)</span><br><span class=\"line\">        self.tensors = tensors</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span><span class=\"params\">(self, index)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> tuple(tensor[index] <span class=\"keyword\">for</span> tensor <span class=\"keyword\">in</span> self.tensors)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.tensors[<span class=\"number\">0</span>].size(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p>例子<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.utils.data <span class=\"keyword\">as</span> Data</span><br><span class=\"line\"></span><br><span class=\"line\">BATCH_SIZE = <span class=\"number\">5</span></span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.linspace(<span class=\"number\">1</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">y = torch.linspace(<span class=\"number\">10</span>, <span class=\"number\">1</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">torch_dataset = Data.TensorDataset(x, y)</span><br><span class=\"line\"></span><br><span class=\"line\">loader = Data.DataLoader(</span><br><span class=\"line\">    dataset=torch_dataset,</span><br><span class=\"line\">    batch_size=BATCH_SIZE,</span><br><span class=\"line\">    shuffle=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">    num_workers=<span class=\"number\">2</span>,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">3</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> step, (batch_x, batch_y) <span class=\"keyword\">in</span> enumerate(loader):</span><br><span class=\"line\">        print(<span class=\"string\">'Epoch: '</span>, epoch, <span class=\"string\">'| Step: '</span>, step, <span class=\"string\">'| batch x: '</span>,</span><br><span class=\"line\">              batch_x.numpy(), <span class=\"string\">'| batch y: '</span>, batch_y.numpy())</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>但貌似这个读取是提前把所有数据都读取到内存中了，如果数据太多的话会不会有点…..</p>\n</blockquote>\n<h2 id=\"数据随机划分\"><a href=\"#数据随机划分\" class=\"headerlink\" title=\"数据随机划分\"></a>数据随机划分</h2><p>上面的猫狗数据集没有进行划分训练集和测试集，官方提供一个函数来对cat_dog的实例进行划分，源码如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Subset</span><span class=\"params\">(Dataset)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Subset of a dataset at specified indices.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">        dataset (Dataset): The whole Dataset</span></span><br><span class=\"line\"><span class=\"string\">        indices (sequence): Indices in the whole set selected for subset</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, dataset, indices)</span>:</span></span><br><span class=\"line\">        self.dataset = dataset</span><br><span class=\"line\">        self.indices = indices</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span><span class=\"params\">(self, idx)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.dataset[self.indices[idx]]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> len(self.indices)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_split</span><span class=\"params\">(dataset, lengths)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Randomly split a dataset into non-overlapping new datasets of given lengths.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">        dataset (Dataset): Dataset to be split</span></span><br><span class=\"line\"><span class=\"string\">        lengths (sequence): lengths of splits to be produced</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> sum(lengths) != len(dataset):</span><br><span class=\"line\">        <span class=\"keyword\">raise</span> ValueError(<span class=\"string\">\"Sum of input lengths does not equal the length of the input dataset!\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    indices = randperm(sum(lengths))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [Subset(dataset, indices[offset - length:offset]) <span class=\"keyword\">for</span> offset, length <span class=\"keyword\">in</span> zip(_accumulate(lengths), lengths)]</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>使用方法：<br>lengths是一个序列，比如10个数据划分为2个测试、8个训练，则传入[2, 8]<br>_accumulate函数的功能：[2, 8]—-&gt;[2, 10]<br>最终返回[Subset子集实例1, Subset子集实例2]，当然也可以传入len(lengths)==3或n，这样划分更多的<br>子集</p>\n</blockquote>\n","permalink":"https://www.ahbzzzh.cn/2018/11/24/pytorch数据读取/","categories":[{"name":"pytorch","slug":"pytorch","permalink":"https://www.ahbzzzh.cn/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://www.ahbzzzh.cn/tags/pytorch/"}]},{"title":"python基础知识点","date":"2018-11-22T09:54:44.000Z","path":"2018/11/22/python基础知识点/","content":"<p><img src=\"https://i.imgur.com/GD4s3il.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<h2 id=\"抽象\"><a href=\"#抽象\" class=\"headerlink\" title=\"抽象\"></a>抽象</h2><h3 id=\"给函数编写文档\"><a href=\"#给函数编写文档\" class=\"headerlink\" title=\"给函数编写文档\"></a>给函数编写文档</h3><p>给函数编写文档，以确保其他人能够理解，可添加注释（以#打头的内容）。还有另一种编写注释的方式，就是添加独立的字符串。放在函数开头的字符串称为文档字符串（docstring），将作为函数的一部分存储起来。<br>例子：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">square</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">'计算给定x的平方值'</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x * x</span><br><span class=\"line\"></span><br><span class=\"line\">print(square(<span class=\"number\">10</span>))</span><br><span class=\"line\">print(square.__doc__)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#out</span></span><br><span class=\"line\"><span class=\"number\">100</span></span><br><span class=\"line\">计算给定x的平方值</span><br></pre></td></tr></table></figure></p>\n<p>数学意义上的函数总是返回根据参数计算得到的结果。在Python中，有些函数什么都不返回。但在Python中，函数就是函数，即使它严格来说并非函数。什么都不返回的函数不包含return语句，或者包含return语句，但没 有在return后面指定值。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'This is printed'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span>   <span class=\"comment\">#return语句只是为了结束函数</span></span><br><span class=\"line\">    print(<span class=\"string\">'This is not'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x = test()</span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出  直接跳过第二个print</span></span><br><span class=\"line\">This <span class=\"keyword\">is</span> printed</span><br><span class=\"line\"><span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"comment\">#python中所有的函数都返回值，如果没有指定返回什么，就将返回None</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"参数魔法\"><a href=\"#参数魔法\" class=\"headerlink\" title=\"参数魔法\"></a>参数魔法</h3><p>这里只介绍收集参数和分配参数</p>\n<h4 id=\"收集参数\"><a href=\"#收集参数\" class=\"headerlink\" title=\"收集参数\"></a>收集参数</h4><p>python允许向函数提供任意数量的参数，只需遵循下面的定义：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_params</span><span class=\"params\">(*params)</span>:</span></span><br><span class=\"line\">    print(params)</span><br><span class=\"line\"></span><br><span class=\"line\">print_params(<span class=\"string\">'bozhou'</span>)</span><br><span class=\"line\">print_params(<span class=\"string\">'bozhou'</span>, <span class=\"string\">'anyi'</span>, <span class=\"string\">'zzh'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">(<span class=\"string\">'bozhou'</span>,)              <span class=\"comment\">#括号里面有个逗号，说明返回的结果为一个元组类型结果</span></span><br><span class=\"line\">(<span class=\"string\">'bozhou'</span>, <span class=\"string\">'anyi'</span>, <span class=\"string\">'zzh'</span>)      <span class=\"comment\">#这个结果就更明显了</span></span><br></pre></td></tr></table></figure></p>\n<p>进一步示例：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_params_2</span><span class=\"params\">(want, *params)</span>:</span></span><br><span class=\"line\">    print(want)</span><br><span class=\"line\">    print(params)</span><br><span class=\"line\"></span><br><span class=\"line\">print_params_2(<span class=\"string\">'hello'</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">hello</span><br><span class=\"line\">(<span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>)  <span class=\"comment\">#参数前带星号，意味着将余下的值收集为一个元组</span></span><br></pre></td></tr></table></figure></p>\n<p>进一步<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">in_the_middle</span><span class=\"params\">(x, *y, z)</span>:</span></span><br><span class=\"line\">    print(x, y, z)</span><br><span class=\"line\"></span><br><span class=\"line\">in_the_middle(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, z=<span class=\"number\">10</span>)    <span class=\"comment\">#也可以放在其他位置，就像本例一样要指定后续参数名称以便区分，否则报错</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\"><span class=\"number\">1</span> (<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>) <span class=\"number\">10</span></span><br></pre></td></tr></table></figure></p>\n<p>收集关键字参数，可使用两个星号<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_params_3</span><span class=\"params\">(**params)</span>:</span></span><br><span class=\"line\">    print(params)</span><br><span class=\"line\"></span><br><span class=\"line\">print_params_3(x=<span class=\"number\">1</span>, y=<span class=\"number\">2</span>, z=<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\">#输出一个字典而不是元组</span></span><br><span class=\"line\">&#123;<span class=\"string\">'z'</span>: <span class=\"number\">3</span>, <span class=\"string\">'x'</span>: <span class=\"number\">1</span>, <span class=\"string\">'y'</span>: <span class=\"number\">2</span>&#125;</span><br></pre></td></tr></table></figure></p>\n<p>进一步可结合使用，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_params_4</span><span class=\"params\">(x, y, z=<span class=\"number\">3</span>, *pospar, **keypar)</span>:</span></span><br><span class=\"line\">    print(x, y, z)</span><br><span class=\"line\">    print(pospar)</span><br><span class=\"line\">    print(keypar)</span><br><span class=\"line\"></span><br><span class=\"line\">print_params_4(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, a=<span class=\"number\">10</span>, b=<span class=\"number\">20</span>)</span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\"><span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span></span><br><span class=\"line\">(<span class=\"number\">4</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">&#123;<span class=\"string\">'a'</span>: <span class=\"number\">10</span>, <span class=\"string\">'b'</span>: <span class=\"number\">20</span>&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"分配参数\"><a href=\"#分配参数\" class=\"headerlink\" title=\"分配参数\"></a>分配参数</h4><p>执行与收集参数相反的操作，用例子来说明什么是相反的操作：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">add</span><span class=\"params\">(x, y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x + y</span><br><span class=\"line\"></span><br><span class=\"line\">params = (<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">print(add(*params))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\"><span class=\"number\">3</span></span><br></pre></td></tr></table></figure></p>\n<p>进一步<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#本例是只在调用的时候使用了星号</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span><span class=\"params\">(name, age)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'name:'</span>, name)</span><br><span class=\"line\">    print(<span class=\"string\">'age:'</span>, age)</span><br><span class=\"line\"></span><br><span class=\"line\">params = &#123;<span class=\"string\">'name'</span>: <span class=\"string\">'zzh'</span>, <span class=\"string\">'age'</span>: <span class=\"number\">25</span>&#125;</span><br><span class=\"line\">fun(**params)</span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">name: zzh</span><br><span class=\"line\">age: <span class=\"number\">25</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#函数定义和调用都使用星号</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun1</span><span class=\"params\">(**params)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'name:'</span>, params[<span class=\"string\">'name'</span>])</span><br><span class=\"line\">    print(<span class=\"string\">'age:'</span>, params[<span class=\"string\">'age'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">fun1(**params)</span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">name: zzh</span><br><span class=\"line\">age: <span class=\"number\">25</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#普通传参</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun1</span><span class=\"params\">(params)</span>:</span></span><br><span class=\"line\">    print(<span class=\"string\">'name:'</span>, params[<span class=\"string\">'name'</span>])</span><br><span class=\"line\">    print(<span class=\"string\">'age:'</span>, params[<span class=\"string\">'age'</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">fun1(params)</span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">name: zzh</span><br><span class=\"line\">age: <span class=\"number\">25</span></span><br></pre></td></tr></table></figure></p>\n<p>本小节中，星号被称为拆分运算符。</p>\n<h3 id=\"作用域\"><a href=\"#作用域\" class=\"headerlink\" title=\"作用域\"></a>作用域</h3><p>作用域：变量存储在作用域（也叫命名空间）中。在Python中，作用域分两大类：全局作用域和局部作用域。作用域可以嵌套。<br>变量到底是什么呢？可将其视为指向值的名称。因此，执行赋值语句x = 1后，名称x指向值1。这几乎与使用字典时一样（字典中的键指向值），只是你使用的是“看不见”的字典。实际上，这种解释已经离真相不远。有一个名为vars的内置函数，它返回这个不可见的字典：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = <span class=\"number\">1</span></span><br><span class=\"line\">scope = vars()</span><br><span class=\"line\">print(type(scope))</span><br><span class=\"line\">print(scope[<span class=\"string\">'x'</span>])</span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">&lt;<span class=\"class\"><span class=\"keyword\">class</span> '<span class=\"title\">dict</span>'&gt;</span></span><br><span class=\"line\"><span class=\"class\">1</span></span><br><span class=\"line\"><span class=\"class\">#这种“看不见的字典”称为命名空间或作用域</span></span><br></pre></td></tr></table></figure></p>\n<p>读取全局变量的值通常不会有问题，但还是存在出现问题的可能性。如果有一个局部<br>变量或参数与你要访问的全局变量同名，就无法直接访问全局变量，因为它被局部变量遮<br>住了。<br>如果需要，可使用函数globals来访问全局变量。这个函数类似于vars，返回一个包含全局变量的字典。（locals返回一个包含局部变量的字典）。当函数内外有两个变量名称相同时，必要时可以使用以下方式在函数内部访问函数外的那个全局变量：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">globals()[<span class=\"string\">'parameter'</span>]</span><br></pre></td></tr></table></figure></p>\n<p>待续<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">## 再谈抽象</span><br><span class=\"line\">开篇定义</span><br><span class=\"line\">多态：可对不同类型的对象执行相同的操作</span><br><span class=\"line\">封装：对外部隐藏有关对象工作原理的细节</span><br><span class=\"line\">继承：可基于通用类创建出专用类</span><br><span class=\"line\">## 对象魔法</span><br><span class=\"line\">### 多态</span><br><span class=\"line\">多态，即多种形态，即便你不知道变量（参数变量）指向的是哪种对象，也能够对其执行操作，且操作的行为将随对象所属的类型（类）而异</span><br><span class=\"line\">示例：</span><br><span class=\"line\">```python</span><br><span class=\"line\">#可假设本例为获取狗狗的体重</span><br><span class=\"line\">def get_something(object):</span><br><span class=\"line\">    if isinstance(object, tuple):          #当以元组的形式传入时，如(&apos;哈士奇&apos;， 50)      </span><br><span class=\"line\">        return object[1]</span><br><span class=\"line\">    #元组的形式过于固定，因为小狗的体重是变化的</span><br><span class=\"line\">    #可以字典的形式传入，将体重单独存在名称为“体重”的key下，这样更加灵活</span><br><span class=\"line\">    elif isinstance(object, dict):       </span><br><span class=\"line\">        return int(object[&apos;height&apos;])</span><br><span class=\"line\">#可以看到一个功能函数，对应两个甚至多个不同类型的对象都可以进行操作</span><br></pre></td></tr></table></figure></p>\n<p>进一步，还有更多的例子：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"string\">'abc'</span>.count(<span class=\"string\">'a'</span>)</span><br><span class=\"line\"><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"string\">'a'</span>].count(<span class=\"string\">'a'</span>)</span><br><span class=\"line\"><span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\">#无需知道它是字符串还是列表就能调用方法count：只要你向这个方法</span></span><br><span class=\"line\">提供一个字符作为参数，它就能正常运行。</span><br></pre></td></tr></table></figure></p>\n<p>不仅仅是函数，python中的运算符也体现出多态的特点，如：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"number\">1</span> + <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"string\">'zzh'</span> + <span class=\"string\">'bozhou'</span></span><br><span class=\"line\"><span class=\"string\">'zzhbozhou'</span></span><br><span class=\"line\">这里讨论的多态形式是Python编程方式的核心，有时称为鸭子类型。这个术语源自如下说法：“如果走起来像鸭子，叫起来像鸭子，那么它就是鸭子。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"封装\"><a href=\"#封装\" class=\"headerlink\" title=\"封装\"></a>封装</h3><p>在程序设计中，封装（Encapsulation）是对具体对象的一种抽象，即将某些部分隐藏起来，在程序外部看不到，其含义是其他程序无法调用。</p>\n<p>封装数据的主要原因是：保护隐私（把不想别人知道的东西封装起来）<br>封装方法的主要原因是：隔离复杂度</p>\n<p>封装其实分为两个层面，但无论哪种层面的封装，都要对外界提供好访问你内部隐藏内容的接口<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">第一个层面的封装（什么都不用做）：创建类和对象会分别创建二者的名称空间，我们只能用类名.或者obj.的方式去访问里面的名字，这本身就是一种封装</span><br><span class=\"line\">第二层次的封装：是指把类中的属性和方法私有化，只供内部使用，也可称为隐藏，但不是真正的隐藏</span><br></pre></td></tr></table></figure></p>\n<p>以上两点下面会提到</p>\n<h3 id=\"隐藏\"><a href=\"#隐藏\" class=\"headerlink\" title=\"隐藏\"></a>隐藏</h3><p>要让方法或属性成为私有的（不能从外部访问），只需让其名称以两个下划线打头即可：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Secretive</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__inaccessible</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        print(<span class=\"string\">\"Bet you can't see me ...\"</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">accessible</span><span class=\"params\">(self)</span>:</span>   </span><br><span class=\"line\">        print(<span class=\"string\">\"The secret message is:\"</span>)</span><br><span class=\"line\">        self.__inaccessible()</span><br><span class=\"line\">o = Secretive()</span><br><span class=\"line\">o.accessible()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">The secret message <span class=\"keyword\">is</span>:</span><br><span class=\"line\">Bet you can<span class=\"string\">'t see me ...</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">#可以看到仍然是可以用其他方式访问的，并不是真正意义上的隐藏</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"继承\"><a href=\"#继承\" class=\"headerlink\" title=\"继承\"></a>继承</h3><p>至于什么是继承，怎么继承这里就不多叙述了…………..<br>我们来看看如何查看一个类是否是另一个类的子类，python已经提供了内置方法，例子如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#类定义就不写了</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>issubclass(son, father)</span><br><span class=\"line\"><span class=\"keyword\">True</span></span><br><span class=\"line\"><span class=\"comment\">#再者如果想查看一个类的基类，可访问其特殊属性__bases__</span></span><br><span class=\"line\">son.__bases__</span><br><span class=\"line\"></span><br><span class=\"line\">要确定一个对象是否是特定类的实例，可使用isinstance，用法同上</span><br></pre></td></tr></table></figure></p>\n<p>进一步<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = son()</span><br><span class=\"line\">#所有son类的对象实例都是类father的对象实例，因为son是father的子类，</span><br><span class=\"line\">这时候使用isinstance可能不是精准的，下面小节中会讲到抽象基类。</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"多个超类\"><a href=\"#多个超类\" class=\"headerlink\" title=\"多个超类\"></a>多个超类</h3><p>一个类的父类可以有多个，如何继承语法形式如下：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#多重继承</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">son</span><span class=\"params\">(father1, father2)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\">```  </span><br><span class=\"line\">需要注意的是：</span><br><span class=\"line\">使用多重继承时，有一点务必注意：如果多个超类以不同的方式实现了同一个方法（即有多个同名方法），必须<span class=\"class\"><span class=\"keyword\">class</span>语句中小心排列这些超类，因为位于前面的类的方法将覆盖位于后面在的类的方法</span></span><br><span class=\"line\"><span class=\"class\"></span></span><br><span class=\"line\"><span class=\"class\">### 接口和内省</span></span><br><span class=\"line\"><span class=\"class\">类中的方法属性都是类的协议接口，多态和封装的思想使我们只关心怎样使用接口，并不关系其中的细节，但<span class=\"title\">python</span>也提供了了解接口的方法，如查看、获取、设置：</span></span><br><span class=\"line\"><span class=\"class\">```<span class=\"title\">python</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">class</span> <span class=\"title\">son</span>:</span></span><br><span class=\"line\">    temp = <span class=\"number\">10</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span><span class=\"params\">(self, a)</span>:</span></span><br><span class=\"line\">        self.name = a </span><br><span class=\"line\">        print(<span class=\"string\">'my name is: '</span>, self.name)</span><br><span class=\"line\"></span><br><span class=\"line\">o = son()</span><br><span class=\"line\">print(hasattr(o, <span class=\"string\">'fun'</span>))</span><br><span class=\"line\">print(getattr(o, <span class=\"string\">'fun'</span>))</span><br><span class=\"line\">print(getattr(o, <span class=\"string\">'fun1'</span>, <span class=\"keyword\">None</span>))</span><br><span class=\"line\">setattr(o, <span class=\"string\">'temp'</span>, <span class=\"number\">7</span>)</span><br><span class=\"line\">print(o.temp)</span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\"><span class=\"keyword\">True</span></span><br><span class=\"line\">&lt;bound method son.fun of &lt;__main__.son object at <span class=\"number\">0x000001CFCF6B1AC8</span>&gt;&gt;</span><br><span class=\"line\"><span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"number\">7</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"抽象基类\"><a href=\"#抽象基类\" class=\"headerlink\" title=\"抽象基类\"></a>抽象基类</h3><p>一般而言，抽象类是不能（至少是不应该）实例化的类，其职责是定义子类应实现或者说必要的一组抽象方法，示例如下：<br>Python通过引入模块abc提供了官方解决方案，这个模块为所谓的抽象基类提供了支持。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> abc <span class=\"keyword\">import</span> ABC, abstractmethod</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Talker</span><span class=\"params\">(ABC)</span>:</span></span><br><span class=\"line\"><span class=\"meta\">    @abstractmethod      #将方法标记为抽象的——在子类中必须实现的方法</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">talk</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">pass</span></span><br><span class=\"line\">抽象类（即包含抽象方法的类）最重要的特征是不能实例化</span><br><span class=\"line\">如果继承的子类没有重写talk方法，则该子类也是抽象类，不能进行实例化否则报错！</span><br><span class=\"line\"><span class=\"comment\">#正确做法</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Knigget</span><span class=\"params\">(Talker)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">talk</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        print(<span class=\"string\">\"Ni!\"</span>)</span><br><span class=\"line\">现在可以进行实例化，而且只有在这种情形下使用isinstance才是比较精确的，</span><br><span class=\"line\">这种情况下，我们可以相信这个实例有对应的方法（如本例talk方法），相对</span><br><span class=\"line\">于手工检测方法抽象基类是个更好的选择</span><br></pre></td></tr></table></figure></p>\n<p>因为在处理编程和对象时，强调构成问题而不是身份问题，强调hasattr函数而不是isinstance函数（而前者是更精确的）</p>\n<h2 id=\"魔法方法、特性和迭代器\"><a href=\"#魔法方法、特性和迭代器\" class=\"headerlink\" title=\"魔法方法、特性和迭代器\"></a>魔法方法、特性和迭代器</h2><h3 id=\"元素访问\"><a href=\"#元素访问\" class=\"headerlink\" title=\"元素访问\"></a>元素访问</h3><p>在Python中，协议通常指的是规范行为的规则。协议指定应实现哪些方法以及这些方法应做什么，类似于接口。序列和映射基本上是元素（item）的集合。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- __len__(self)：这个方法应返回集合包含的项数，对序列来说为元素个数，对映射来说为键-值对数</span><br><span class=\"line\">- __getitem__(self, key)：这个方法应返回与指定键相关联的值。对序列来说，键应该是<span class=\"number\">0</span>~n<span class=\"number\">-1</span>的整数，其中n为序列的长度</span><br><span class=\"line\">- __setitem__(self, key, value)：这个方法应以与键相关联的方式存储值，以便以后能够使用__getitem__来获取</span><br><span class=\"line\">- __delitem__(self, key)：这个方法在对对象的组成部分使用__del__语句时被调用，应删除与key相关联的值</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">temp</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, value)</span>:</span></span><br><span class=\"line\">        self.value = value</span><br><span class=\"line\">        self.seq = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> len(self.seq)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span><span class=\"params\">(self, key)</span>:</span></span><br><span class=\"line\">        print(<span class=\"string\">'i am getitem!'</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.seq[key]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__setitem__</span><span class=\"params\">(self, key, value)</span>:</span></span><br><span class=\"line\">        print(<span class=\"string\">'i am setitem!'</span>)</span><br><span class=\"line\">        self.seq[key] = self.value</span><br><span class=\"line\"></span><br><span class=\"line\">b = temp(<span class=\"number\">100</span>)</span><br><span class=\"line\">b[<span class=\"number\">1</span>] = <span class=\"number\">10</span>                <span class=\"comment\">#对self.[key]进行赋值操作时调用__getitem__</span></span><br><span class=\"line\">print(b[<span class=\"number\">1</span>])              <span class=\"comment\">#以self[key]的方式获取值时调用__setitem__                        </span></span><br><span class=\"line\">print(len(b)) </span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"comment\">#输出、由输出可看到调用了哪个方法</span></span><br><span class=\"line\">i am setitem!</span><br><span class=\"line\">i am getitem!</span><br><span class=\"line\"><span class=\"number\">100</span></span><br><span class=\"line\"><span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"property\"><a href=\"#property\" class=\"headerlink\" title=\"property\"></a>property</h3><p>示例：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Rectangle</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span> <span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.width = <span class=\"number\">0</span></span><br><span class=\"line\">        self.height = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">set_size</span><span class=\"params\">(self, size)</span>:</span></span><br><span class=\"line\">        self.width, self.height = size</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_size</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.width, self.height</span><br><span class=\"line\">    </span><br><span class=\"line\">r = Rectangle()</span><br><span class=\"line\">r.width = <span class=\"number\">10</span></span><br><span class=\"line\">r.height = <span class=\"number\">5</span></span><br><span class=\"line\">print(r.get_size())</span><br><span class=\"line\">print(r.set_size((<span class=\"number\">10</span>, <span class=\"number\">20</span>)))</span><br><span class=\"line\">print(r.width)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">(<span class=\"number\">10</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"><span class=\"keyword\">None</span></span><br><span class=\"line\"><span class=\"number\">10</span></span><br></pre></td></tr></table></figure></p>\n<p>使用property函数<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">通过调用函数property并将存取方法作为参数（获取方法在前，设置方法在后）</span><br><span class=\"line\">创建了一个新特性（如果我们需要这个特性的话，property给我提供了一个方便</span><br><span class=\"line\">的实现，否则我们还要重新写类方法），然后将名称size关联到这个特性；</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Rectangle</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span> <span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.width = <span class=\"number\">0</span></span><br><span class=\"line\">        self.height = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">set_size</span><span class=\"params\">(self, size)</span>:</span></span><br><span class=\"line\">        self.width, self.height = size</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_size</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.width, self.height</span><br><span class=\"line\">    size = property(get_size, set_size)    </span><br><span class=\"line\">    </span><br><span class=\"line\">r = Rectangle()</span><br><span class=\"line\">r.width = <span class=\"number\">10</span></span><br><span class=\"line\">r.height = <span class=\"number\">5</span></span><br><span class=\"line\">print(r.size)               <span class=\"comment\">#r.size会触发get_size方法</span></span><br><span class=\"line\">r.size = <span class=\"number\">150</span>, <span class=\"number\">100</span>     <span class=\"comment\">#赋值会触发set_size方法</span></span><br><span class=\"line\">print(r.width)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">(<span class=\"number\">10</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\"><span class=\"number\">150</span></span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">property形式</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">property</span><span class=\"params\">([fget[, fset[, fdel[, doc]]]])</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">fdel</span>参数 -- 删除属性值函数</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"title\">doc</span>参数 -- 属性描述信息</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/10030401-6f79cf38e1b3a76c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/660\" alt=\"\"></p>\n<h3 id=\"访问控制\"><a href=\"#访问控制\" class=\"headerlink\" title=\"访问控制\"></a>访问控制</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">__getattr__(self, name)</span><br><span class=\"line\">当用户试图访问一个根本不存在（或者暂时不存在）的属性时，你可以通过这个魔</span><br><span class=\"line\">法方法来定义类的行为。这个可以用于捕捉错误的拼写并且给出指引，使用废弃属</span><br><span class=\"line\">性时给出警告（如果你愿意，仍然可以计算并且返回该属性），以及灵活地处理属</span><br><span class=\"line\">性错误。只有当试图访问不存在的属性时它才会被调用，所以这不能算是一个真正</span><br><span class=\"line\">的封装的办法</span><br><span class=\"line\">------------------------------------------------------------------------</span><br><span class=\"line\">__setattr__(self, name, value)</span><br><span class=\"line\">和 __getattr__ 不同， __setattr__ 可以用于真正意义上的封装。它允许你自定义某</span><br><span class=\"line\">个属性的赋值行为，不管这个属性存在与否，也就是说你可以对任意属性的任何变</span><br><span class=\"line\">化都定义自己的规则。然后，一定要小心使用 __setattr__ ，这个列表最后的例子</span><br><span class=\"line\">中会有所展示</span><br><span class=\"line\">-------------------------------------------------------------------------</span><br><span class=\"line\">__getattribute__(self, name)：在属性被访问时自动调用</span><br><span class=\"line\">-------------------------------------------------------------------------</span><br><span class=\"line\">__delattr__(self, name)：试图删除属性时自动调用</span><br></pre></td></tr></table></figure>\n<h3 id=\"迭代器\"><a href=\"#迭代器\" class=\"headerlink\" title=\"迭代器\"></a>迭代器</h3><p>迭代（iterate）意味着重复多次，就像循环那样。已知的for循环迭代序列和字典，但实际上也可迭代其他对象：实现了方法<strong> iter</strong>的对象；<br>方法<strong> iter</strong>返回一个迭代器(可迭代对象)，前提是该对象包含方法<strong> next</strong> ;当你调用方法<strong> next</strong>时，迭代器应返回其下一个值。如果迭代器没有可供返回的值，应引发StopIteration异常；<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">使用 iter() 函数调用，以及在类似 <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> container: 的循环时被调用</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Fibs</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.a = <span class=\"number\">0</span></span><br><span class=\"line\">        self.b = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__next__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        print(<span class=\"string\">\"b\"</span>)</span><br><span class=\"line\">        self.a, self.b = self.b, self.a + self.b</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.a</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__iter__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        print(<span class=\"string\">\"a\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self</span><br><span class=\"line\"></span><br><span class=\"line\">b = Fibs()</span><br><span class=\"line\">print(<span class=\"string\">\"start!\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> b:</span><br><span class=\"line\">    print(<span class=\"string\">\"d\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> f &gt; <span class=\"number\">2</span>:</span><br><span class=\"line\">        print(<span class=\"string\">\"mark\"</span>)</span><br><span class=\"line\">        print(f)</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出，通过输出字母的顺便来看是方法触发的顺序</span></span><br><span class=\"line\">start!</span><br><span class=\"line\">a</span><br><span class=\"line\">b</span><br><span class=\"line\">d</span><br><span class=\"line\">b</span><br><span class=\"line\">d</span><br><span class=\"line\">b</span><br><span class=\"line\">d</span><br><span class=\"line\">b</span><br><span class=\"line\">d</span><br><span class=\"line\">mark</span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\">迭代与使用列表相比，在某些情况下，更简洁；列表可能会占有过多的内存，而有</span><br><span class=\"line\">时候我们想逐个的取值，再者如上例所示斐波那契数列是无穷大的，使用列表不合</span><br><span class=\"line\">适;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"生成器\"><a href=\"#生成器\" class=\"headerlink\" title=\"生成器\"></a>生成器</h3><p>生成器是一种使用普通函数语法定义的迭代，当Python 函数不用return 返回值，用yield关键字的时候，函数的返回值为生成器对象；在 for 循环执行时，每次循环都会执行 fab 函数内部的代码，执行到 yield b 时，fab 函数就返回一个迭代值，下次迭代时，代码从 yield b 的下一条语句继续执行，而函数的本地变量看起来和上次中断执行前是完全一样的，于是函数继续执行，直到再次遇到 yield。<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">创建生成器</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fab</span><span class=\"params\">(max)</span>:</span> </span><br><span class=\"line\">    n, a, b = <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span> </span><br><span class=\"line\">    <span class=\"keyword\">while</span> n &lt; max: </span><br><span class=\"line\">        <span class=\"keyword\">yield</span> b      <span class=\"comment\"># 使用 yield</span></span><br><span class=\"line\">        <span class=\"comment\"># print b </span></span><br><span class=\"line\">        a, b = b, a + b </span><br><span class=\"line\">        n = n + <span class=\"number\">1</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"keyword\">for</span> n <span class=\"keyword\">in</span> fab(<span class=\"number\">5</span>): </span><br><span class=\"line\">    <span class=\"keyword\">print</span> n</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\"><span class=\"number\">1</span> </span><br><span class=\"line\"><span class=\"number\">1</span> </span><br><span class=\"line\"><span class=\"number\">2</span> </span><br><span class=\"line\"><span class=\"number\">3</span> </span><br><span class=\"line\"><span class=\"number\">5</span></span><br></pre></td></tr></table></figure></p>\n<p>另一种创建形式，生成器表达式<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g = ((i + <span class=\"number\">2</span>) ** <span class=\"number\">2</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>, <span class=\"number\">27</span>))</span><br><span class=\"line\">print(next(g))</span><br><span class=\"line\">print(next(g))</span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\"><span class=\"number\">16</span></span><br><span class=\"line\"><span class=\"number\">25</span></span><br></pre></td></tr></table></figure></p>\n","permalink":"https://www.ahbzzzh.cn/2018/11/22/python基础知识点/","categories":[{"name":"python","slug":"python","permalink":"https://www.ahbzzzh.cn/categories/python/"}],"tags":[{"name":"python基础知识点","slug":"python基础知识点","permalink":"https://www.ahbzzzh.cn/tags/python基础知识点/"}]},{"title":"生活感悟","date":"2018-11-22T08:25:11.000Z","path":"2018/11/22/生活感悟/","content":"<p><img src=\"https://i.imgur.com/58EYVfW.png\" alt=\"\"><br><a id=\"more\"></a></p>\n","permalink":"https://www.ahbzzzh.cn/2018/11/22/生活感悟/","categories":[],"tags":[]},{"title":"C++日记","date":"2018-11-22T08:24:38.000Z","path":"2018/11/22/C-日记/","content":"<p><img src=\"https://i.imgur.com/hCe9e1r.png\" alt=\"\"><br><a id=\"more\"></a></p>\n","permalink":"https://www.ahbzzzh.cn/2018/11/22/C-日记/","categories":[],"tags":[{"name":"C++","slug":"C","permalink":"https://www.ahbzzzh.cn/tags/C/"}]},{"title":"torchvision.transforms模块介绍","date":"2018-11-21T11:42:51.000Z","path":"2018/11/21/torchvision-transforms模块介绍/","content":"<p><img src=\"https://i.imgur.com/H6o8MuU.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<blockquote>\n<p>transforms是torchvision中常规的图像变换模块，训练模型时基本上都会用到这些操作来进行数据增强~<br>模块学习还是尽量去看官方文档，但文档有些简略，这时候就要看源码了</p>\n</blockquote>\n<h2 id=\"常规的图像变换操作\"><a href=\"#常规的图像变换操作\" class=\"headerlink\" title=\"常规的图像变换操作\"></a>常规的图像变换操作</h2><h3 id=\"Compose\"><a href=\"#Compose\" class=\"headerlink\" title=\"Compose\"></a>Compose</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">Compose</span><span class=\"params\">(transforms)</span></span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>参数：包含Transform对象的列表<br>官方文档没说能返回什么…..翻开源码</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Compose</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Composes several transforms together.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        transforms (list of ``Transform`` objects): list of transforms to compose.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Example:</span></span><br><span class=\"line\"><span class=\"string\">        &gt;&gt;&gt; transforms.Compose([</span></span><br><span class=\"line\"><span class=\"string\">        &gt;&gt;&gt;     transforms.CenterCrop(10),</span></span><br><span class=\"line\"><span class=\"string\">        &gt;&gt;&gt;     transforms.ToTensor(),</span></span><br><span class=\"line\"><span class=\"string\">        &gt;&gt;&gt; ])</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, transforms)</span>:</span></span><br><span class=\"line\">        self.transforms = transforms</span><br><span class=\"line\">\t<span class=\"comment\"># 返回进行所有变换后的图像</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__call__</span><span class=\"params\">(self, img)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> self.transforms:</span><br><span class=\"line\">            img = t(img)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> img</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__repr__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        format_string = self.__class__.__name__ + <span class=\"string\">'('</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> self.transforms:</span><br><span class=\"line\">            format_string += <span class=\"string\">'\\n'</span></span><br><span class=\"line\">            format_string += <span class=\"string\">'    &#123;0&#125;'</span>.format(t)</span><br><span class=\"line\">        format_string += <span class=\"string\">'\\n)'</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> format_string</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><strong>call</strong>可以使得类实例通过传递相应参数像调用函数一样来使用<br><strong>repr</strong>功能是用于显示类实例的属性信息等</p>\n</blockquote>\n<h3 id=\"CenterCrop中心裁剪\"><a href=\"#CenterCrop中心裁剪\" class=\"headerlink\" title=\"CenterCrop中心裁剪\"></a>CenterCrop中心裁剪</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">CenterCrop</span><span class=\"params\">(size)</span></span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>参数：size是希望裁剪后输出的尺寸，值为一个序列或者一个int整数，序列比如(200,100)<br>为int值a时，就等价于(a,a)</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> . <span class=\"keyword\">import</span> functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CenterCrop</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Crops the given PIL Image at the center.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Args:</span></span><br><span class=\"line\"><span class=\"string\">        size (sequence or int): Desired output size of the crop. If size is an</span></span><br><span class=\"line\"><span class=\"string\">            int instead of sequence like (h, w), a square crop (size, size) is</span></span><br><span class=\"line\"><span class=\"string\">            made.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, size)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> isinstance(size, numbers.Number):</span><br><span class=\"line\">            self.size = (int(size), int(size))</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            self.size = size</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__call__</span><span class=\"params\">(self, img)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">            img (PIL Image): Image to be cropped.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            PIL Image: Cropped image.</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> F.center_crop(img, self.size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__repr__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.__class__.__name__ + <span class=\"string\">'(size=&#123;0&#125;)'</span>.format(self.size)</span><br><span class=\"line\"><span class=\"comment\"># 实际上这些图像变换功能的函数都在functional模块中</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">center_crop</span><span class=\"params\">(img, output_size)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> isinstance(output_size, numbers.Number):</span><br><span class=\"line\">        output_size = (int(output_size), int(output_size))</span><br><span class=\"line\">    w, h = img.size</span><br><span class=\"line\">    th, tw = output_size</span><br><span class=\"line\">    i = int(round((h - th) / <span class=\"number\">2.</span>))</span><br><span class=\"line\">    j = int(round((w - tw) / <span class=\"number\">2.</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> crop(img, i, j, th, tw)</span><br></pre></td></tr></table></figure>\n<h3 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h3><blockquote>\n<p>可以看到实际上这些图像变换功能的函数都在functional模块中，而Transform对象也大致都有<strong>init</strong>、<br><strong>call</strong>、<strong>repr</strong>这几个’魔法’函数组成</p>\n</blockquote>\n<p>下面就只进行简单介绍、认识一下…<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">ColorJitter</span><span class=\"params\">(brightness=<span class=\"number\">0</span>, contrast=<span class=\"number\">0</span>, saturation=<span class=\"number\">0</span>, hue=<span class=\"number\">0</span>)</span></span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>随机亮度、对比度、饱和度调整，注意是随机采样的<br>参数：</p>\n<ul>\n<li>brightness：均匀随机采样，区间[max(0, 1 - brightness), 1 + brightness].</li>\n<li>contrast：均匀随机采样，区间[max(0, 1 - contrast), 1 + contrast].</li>\n<li>saturation：同理.</li>\n<li>hue (float) – How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue]<br>Should be &gt;=0 and &lt;= 0.5.</li>\n</ul>\n</blockquote>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">FiveCrop</span><span class=\"params\">(size)</span></span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>对给定PIL图像进行四角裁剪和中心裁剪<br>参数：size值和上面中心裁剪同理<br>注意：这个操作会造成输入和标签的不匹配，因为你给定n个图像返回5×n个结果~官方也给出了解决办法<br>在TSN算法(行为识别)看到过这种用法，第一次见~~不是很明白这种操作</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>transform = Compose([</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>   FiveCrop(size), <span class=\"comment\"># this is a list of PIL Images</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>   Lambda(<span class=\"keyword\">lambda</span> crops: torch.stack([ToTensor()(crop) <span class=\"keyword\">for</span> crop <span class=\"keyword\">in</span> crops])) <span class=\"comment\"># returns a 4D tensor</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>])</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"comment\">#In your test loop you can do the following:</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>input, target = batch <span class=\"comment\"># input is a 5d tensor, target is 2d</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>bs, ncrops, c, h, w = input.size()</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = model(input.view(<span class=\"number\">-1</span>, c, h, w)) <span class=\"comment\"># 传入的时候融合batch和裁剪维度</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result_avg = result.view(bs, ncrops, <span class=\"number\">-1</span>).mean(<span class=\"number\">1</span>) <span class=\"comment\"># 对结果，对所有样本的ncrops取平均，所以由此看来，尽管是返回五倍的数据，但对最后的结果是汇总的</span></span><br></pre></td></tr></table></figure>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">Grayscale</span><span class=\"params\">(num_output_channels=<span class=\"number\">1</span>)</span></span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>转换图像为灰度图像<br>参数：1或3<br>返回：灰度图像，当为1时，返回一个单通道图像；为3时，返回通道数为3的图像(r == g == b)<br>返回图像类型是PIL</p>\n</blockquote>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">RandomCrop</span><span class=\"params\">(size, padding=None, pad_if_needed=False, fill=<span class=\"number\">0</span>, padding_mode=<span class=\"string\">'constant'</span>)</span></span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>随机裁剪<br>参数：</p>\n<ul>\n<li>size：输出尺寸</li>\n<li>padding：对图像上下作用进行填充，手动填充….</li>\n<li>pad_if_needed：True时，若需要输出比输入尺寸大，则进行自动填充<br>后面两个参数是官方文档写的，但源码没有这两个参数，pytorch版本是一致的，不知道为什么~</li>\n</ul>\n</blockquote>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">RandomRotation</span><span class=\"params\">(degrees, resample=False, expand=False, center=None)</span></span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>随机角度旋转<br>参数：</p>\n<ul>\n<li>degrees：角度范围，(min, max)或者(-degrees, +degrees)</li>\n<li>resample：重采样，({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional)</li>\n<li>expand：为False时，旋转后超出尺寸区域就略去；为True时，则扩展图像尺寸使其完全包含原始图像</li>\n<li>center:旋转中心，默认是图像中心</li>\n</ul>\n</blockquote>\n<hr>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torchvision</span>.<span class=\"title\">transforms</span>.<span class=\"title\">Normalize</span><span class=\"params\">(mean, std)</span></span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>根据给定的均值和方差对tensor类型图像进行规范化</p>\n</blockquote>\n<h3 id=\"Conversion-Transforms\"><a href=\"#Conversion-Transforms\" class=\"headerlink\" title=\"Conversion Transforms\"></a>Conversion Transforms</h3><blockquote>\n<p>Convert a tensor or an ndarray to PIL Image.<br>class torchvision.transforms.ToPILImage(mode=None)<br>class torchvision.transforms.ToTensor<br>Convert a PIL Image or numpy.ndarray to tensor.</p>\n</blockquote>\n<h3 id=\"Generic-Transforms\"><a href=\"#Generic-Transforms\" class=\"headerlink\" title=\"Generic Transforms\"></a>Generic Transforms</h3><blockquote>\n<p>class torchvision.transforms.Lambda(lambd)<br>将用户自定义的函数作为一个transform，函数中当然也可以使用transform操作，为用户提供一定的灵活度<br>参数：lambd函数</p>\n</blockquote>\n<h2 id=\"举例\"><a href=\"#举例\" class=\"headerlink\" title=\"举例\"></a>举例</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> T</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"></span><br><span class=\"line\">transfer = T.Compose([</span><br><span class=\"line\">    T.RandomRotation(<span class=\"number\">50</span>),</span><br><span class=\"line\">    T.RandomCrop(size=<span class=\"number\">400</span>, padding=<span class=\"number\">0</span>, pad_if_needed=<span class=\"keyword\">True</span>),</span><br><span class=\"line\">    T.RandomGrayscale(p=<span class=\"number\">0.6</span>),</span><br><span class=\"line\">    ])</span><br><span class=\"line\"></span><br><span class=\"line\">image = Image.open(<span class=\"string\">\"E:/temp/czym.jpg\"</span>)</span><br><span class=\"line\"><span class=\"comment\"># image = cv2.imread('E:/temp/200.jpg')</span></span><br><span class=\"line\"><span class=\"comment\"># image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))</span></span><br><span class=\"line\">image_transfered = transfer(image)</span><br><span class=\"line\">image_transfered.show()</span><br></pre></td></tr></table></figure>\n<p>原始图像：<br><img src=\"https://i.imgur.com/9n14yfU.jpg\" alt=\"原始图像\"><br>转换后图像：<br><img src=\"https://i.imgur.com/YecvFTP.png\" alt=\"转换后的图像\"></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><a href=\"https://pytorch.org/docs/stable/torchvision/transforms.html\" target=\"_blank\" rel=\"noopener\">https://pytorch.org/docs/stable/torchvision/transforms.html</a></li>\n</ul>\n","permalink":"https://www.ahbzzzh.cn/2018/11/21/torchvision-transforms模块介绍/","categories":[{"name":"pytorch","slug":"pytorch","permalink":"https://www.ahbzzzh.cn/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://www.ahbzzzh.cn/tags/pytorch/"}]},{"title":"pytorch入门文档","date":"2018-11-21T03:31:03.000Z","path":"2018/11/21/pytorch入门文档/","content":"<p><img src=\"https://i.imgur.com/bbjOLfM.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<blockquote>\n<p>GETTING STARTED<br>对入门文档的翻译和注释，链接在参考文献中</p>\n</blockquote>\n<h2 id=\"Tensors\"><a href=\"#Tensors\" class=\"headerlink\" title=\"Tensors\"></a>Tensors</h2><p>pytorch深度学习框架中的Tensor和numpy的操作类似，还可以在GPU上进行加速~<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Construct a 5x3 matrix, uninitialized</span></span><br><span class=\"line\"><span class=\"comment\"># 刚开始不明白这个未初始化是什么意思，明明赋值了啊，后来试验了多次</span></span><br><span class=\"line\"><span class=\"comment\"># 发现每次打印出来的值都不一样，有的非常大有的非常小，可能这里未初始化的意思</span></span><br><span class=\"line\"><span class=\"comment\"># 就是指没有规律的意思吧</span></span><br><span class=\"line\">x = torch.empty(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([[<span class=\"number\">8.3665e+22</span>, <span class=\"number\">4.5580e-41</span>, <span class=\"number\">1.6025e-03</span>],</span><br><span class=\"line\">        [<span class=\"number\">3.0763e-41</span>, <span class=\"number\">0.0000e+00</span>, <span class=\"number\">0.0000e+00</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.0000e+00</span>, <span class=\"number\">0.0000e+00</span>, <span class=\"number\">3.4438e-41</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.0000e+00</span>, <span class=\"number\">4.8901e-36</span>, <span class=\"number\">2.8026e-45</span>],</span><br><span class=\"line\">        [<span class=\"number\">6.6121e+31</span>, <span class=\"number\">0.0000e+00</span>, <span class=\"number\">9.1084e-44</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">-------------------------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 随机初始化</span></span><br><span class=\"line\"><span class=\"comment\"># 从[0，1)的均匀分布中采样</span></span><br><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([[<span class=\"number\">0.1607</span>, <span class=\"number\">0.0298</span>, <span class=\"number\">0.7555</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.8887</span>, <span class=\"number\">0.1625</span>, <span class=\"number\">0.6643</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.7328</span>, <span class=\"number\">0.5419</span>, <span class=\"number\">0.6686</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.0793</span>, <span class=\"number\">0.1133</span>, <span class=\"number\">0.5956</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.3149</span>, <span class=\"number\">0.9995</span>, <span class=\"number\">0.6372</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">-------------------------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建值全部为0的tensor，并定义数值类型</span></span><br><span class=\"line\">x = torch.zeros(<span class=\"number\">5</span>, <span class=\"number\">3</span>, dtype=torch.long)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>],</span><br><span class=\"line\">        [<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">-------------------------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 指定data定义tensor</span></span><br><span class=\"line\">x = torch.tensor([<span class=\"number\">5.5</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([<span class=\"number\">5.5000</span>, <span class=\"number\">3.0000</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">-------------------------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 基于已存在tensor来创建tensor(数值不一定相同，但size一定是相同的)，这些方法可以重用输入tensor的性质，除非你重新指定，比如类型</span></span><br><span class=\"line\">x = x.new_ones(<span class=\"number\">5</span>, <span class=\"number\">3</span>, dtype=torch.double)      <span class=\"comment\"># new_* methods take in sizes</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.randn_like(x, dtype=torch.float)    <span class=\"comment\"># override dtype!  被新的类型覆盖</span></span><br><span class=\"line\">print(x)                                      <span class=\"comment\"># result has the same size</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([[<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>],</span><br><span class=\"line\">        [<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>],</span><br><span class=\"line\">        [<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>],</span><br><span class=\"line\">        [<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>],</span><br><span class=\"line\">        [<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>]], dtype=torch.float64)</span><br><span class=\"line\">tensor([[<span class=\"number\">-0.2217</span>, <span class=\"number\">-0.9135</span>, <span class=\"number\">-0.6010</span>],</span><br><span class=\"line\">        [<span class=\"number\">-0.3193</span>, <span class=\"number\">-0.3675</span>,  <span class=\"number\">0.1951</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.0646</span>, <span class=\"number\">-0.4947</span>,  <span class=\"number\">1.0374</span>],</span><br><span class=\"line\">        [<span class=\"number\">-0.4154</span>, <span class=\"number\">-1.0247</span>, <span class=\"number\">-1.2872</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.5228</span>,  <span class=\"number\">0.3420</span>,  <span class=\"number\">0.0219</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">-------------------------------------------------------------------------------------</span><br><span class=\"line\"><span class=\"comment\"># 获取size</span></span><br><span class=\"line\">print(x.size())</span><br><span class=\"line\">s = x.size()</span><br><span class=\"line\">print(s)</span><br><span class=\"line\">print(s[<span class=\"number\">0</span>])        <span class=\"comment\"># torch.Size实际上是一个元组，可以执行元组的所有操作</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">torch.Size([<span class=\"number\">5</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\"><span class=\"number\">5</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Operations\"><a href=\"#Operations\" class=\"headerlink\" title=\"Operations\"></a>Operations</h3><p>这里介绍一下tensor的操作<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 以加法举例</span></span><br><span class=\"line\">x = torch.ones(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">y = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">print(x + y)</span><br><span class=\"line\">print(torch.add(x, y))</span><br><span class=\"line\"></span><br><span class=\"line\">result = torch.empty(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\">torch.add(x, y, out=result)     <span class=\"comment\"># 将输出结果赋给result</span></span><br><span class=\"line\">print(result)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#in-place</span></span><br><span class=\"line\"><span class=\"comment\"># adds x to y</span></span><br><span class=\"line\"><span class=\"comment\"># 其实y.add(x)也是可以相加的，但“_”的标识符意味着y的值会被改变</span></span><br><span class=\"line\">y.add_(x)</span><br><span class=\"line\">print(y)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#以上所有打印的输出都是一样的</span></span><br><span class=\"line\">tensor([[ <span class=\"number\">0.2349</span>, <span class=\"number\">-0.0427</span>, <span class=\"number\">-0.5053</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6455</span>,  <span class=\"number\">0.1199</span>,  <span class=\"number\">0.4239</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.1279</span>,  <span class=\"number\">0.1105</span>,  <span class=\"number\">1.4637</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.4259</span>, <span class=\"number\">-0.0763</span>, <span class=\"number\">-0.9671</span>],</span><br><span class=\"line\">        [ <span class=\"number\">0.6856</span>,  <span class=\"number\">0.5047</span>,  <span class=\"number\">0.4250</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">-------------------------------------------------------------------------------</span><br><span class=\"line\"><span class=\"comment\"># torch.view是用来resize/reshape tensor</span></span><br><span class=\"line\">x = torch.randn(<span class=\"number\">4</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\">y = x.view(<span class=\"number\">16</span>)</span><br><span class=\"line\">z = x.view(<span class=\"number\">-1</span>, <span class=\"number\">8</span>)  <span class=\"comment\"># the size -1 is inferred from other dimensions</span></span><br><span class=\"line\">print(x.size(), y.size(), z.size())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">torch.Size([<span class=\"number\">4</span>, <span class=\"number\">4</span>]) torch.Size([<span class=\"number\">16</span>]) torch.Size([<span class=\"number\">2</span>, <span class=\"number\">8</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果tensor仅有一个元素，使用.item()可以把值取出来</span></span><br><span class=\"line\">x = torch.randn(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(x)</span><br><span class=\"line\">print(x.item())</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([<span class=\"number\">1.9218</span>])</span><br><span class=\"line\"><span class=\"number\">1.9218417406082153</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<h2 id=\"NUMPY和Tenosr互转\"><a href=\"#NUMPY和Tenosr互转\" class=\"headerlink\" title=\"NUMPY和Tenosr互转\"></a>NUMPY和Tenosr互转</h2><p>Torch Tensor 和 NumPy array会共享底层内存位置，所以改变其中一个另一个值也会改变，举例：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a = torch.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">b = a.numpy()         <span class=\"comment\"># tenor----&gt;numpy</span></span><br><span class=\"line\">print(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>])</span><br><span class=\"line\">[<span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span> <span class=\"number\">1.</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">----------------------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 改变tensor</span></span><br><span class=\"line\">a.add_(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)          <span class=\"comment\"># array也会改变</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([<span class=\"number\">2.</span>, <span class=\"number\">2.</span>, <span class=\"number\">2.</span>, <span class=\"number\">2.</span>, <span class=\"number\">2.</span>])</span><br><span class=\"line\">[<span class=\"number\">2.</span> <span class=\"number\">2.</span> <span class=\"number\">2.</span> <span class=\"number\">2.</span> <span class=\"number\">2.</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">----------------------------------------------------------------------------------</span><br><span class=\"line\"></span><br><span class=\"line\">a = np.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = torch.from_numpy(a)          <span class=\"comment\"># numpy------&gt;tensor</span></span><br><span class=\"line\">np.add(a, <span class=\"number\">1</span>, out=a)</span><br><span class=\"line\">print(a)</span><br><span class=\"line\">print(b)        tensor也跟着改变</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">[<span class=\"number\">2.</span> <span class=\"number\">2.</span> <span class=\"number\">2.</span> <span class=\"number\">2.</span> <span class=\"number\">2.</span>]</span><br><span class=\"line\">tensor([<span class=\"number\">2.</span>, <span class=\"number\">2.</span>, <span class=\"number\">2.</span>, <span class=\"number\">2.</span>, <span class=\"number\">2.</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h2 id=\"CUDA-TENSORS\"><a href=\"#CUDA-TENSORS\" class=\"headerlink\" title=\"CUDA TENSORS\"></a>CUDA TENSORS</h2><p>使用.to方法把tensor移送到cpu或者gpu上<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>)</span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    device = torch.device(<span class=\"string\">\"cuda\"</span>)          <span class=\"comment\"># a CUDA device object</span></span><br><span class=\"line\">    y = torch.ones_like(x, device=device)  <span class=\"comment\"># directly create a tensor on GPU </span></span><br><span class=\"line\">    <span class=\"comment\"># or just use strings ``.to(\"cuda\")``</span></span><br><span class=\"line\">    <span class=\"comment\"># 相当于x.to(\"cuda\")</span></span><br><span class=\"line\">    x = x.to(device)                       </span><br><span class=\"line\">    z = x + y</span><br><span class=\"line\">    print(z)</span><br><span class=\"line\">    <span class=\"comment\"># 将结果送到cpu上，还可以改变类型</span></span><br><span class=\"line\">    print(z.to(<span class=\"string\">\"cpu\"</span>, torch.double)) </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">tensor([<span class=\"number\">1.4118</span>, <span class=\"number\">1.2739</span>, <span class=\"number\">1.6951</span>, <span class=\"number\">1.7109</span>, <span class=\"number\">1.4069</span>], device=<span class=\"string\">'cuda:0'</span>)</span><br><span class=\"line\">tensor([<span class=\"number\">1.4118</span>, <span class=\"number\">1.2739</span>, <span class=\"number\">1.6951</span>, <span class=\"number\">1.7109</span>, <span class=\"number\">1.4069</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure></p>\n<hr>\n<h2 id=\"NEURAL-NETWORKS\"><a href=\"#NEURAL-NETWORKS\" class=\"headerlink\" title=\"NEURAL NETWORKS\"></a>NEURAL NETWORKS</h2><p>定义一个网络，例子：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        super(Net, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class=\"line\">        <span class=\"comment\"># kernel</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        <span class=\"comment\"># an affine operation: y = Wx + b</span></span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>, <span class=\"number\">120</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># Max pooling over a (2, 2) window</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class=\"number\">2</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">        <span class=\"comment\"># If the size is a square you can only specify a single number</span></span><br><span class=\"line\">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class=\"number\">2</span>)</span><br><span class=\"line\">        x = x.view(<span class=\"number\">-1</span>, self.num_flat_features(x))</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        x = self.fc3(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">num_flat_features</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        size = x.size()[<span class=\"number\">1</span>:]  <span class=\"comment\"># all dimensions except the batch dimension</span></span><br><span class=\"line\">        num_features = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> size:</span><br><span class=\"line\">            num_features *= s</span><br><span class=\"line\">        <span class=\"keyword\">return</span> num_features</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">net = Net()</span><br><span class=\"line\">print(net)</span><br><span class=\"line\"></span><br><span class=\"line\">params = list(net.parameters())</span><br><span class=\"line\">print(len(params))       <span class=\"comment\"># w-b-w-b~~~~~依次，五层，所以输出为10</span></span><br><span class=\"line\">print(params[<span class=\"number\">0</span>].size())  <span class=\"comment\"># conv1's .weight</span></span><br><span class=\"line\">print(params[<span class=\"number\">1</span>].size())  <span class=\"comment\"># 偏置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 利用伪数据进行前向推断，维度依次为batchsize、图像通道、长宽</span></span><br><span class=\"line\">input = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>)    </span><br><span class=\"line\">out = net(input)</span><br><span class=\"line\">print(out)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\"><span class=\"comment\"># 直接就可以打印出每一层信息</span></span><br><span class=\"line\">Net(</span><br><span class=\"line\">  (conv1): Conv2d(<span class=\"number\">1</span>, <span class=\"number\">6</span>, kernel_size=(<span class=\"number\">5</span>, <span class=\"number\">5</span>), stride=(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">  (conv2): Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, kernel_size=(<span class=\"number\">5</span>, <span class=\"number\">5</span>), stride=(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">  (fc1): Linear(in_features=<span class=\"number\">400</span>, out_features=<span class=\"number\">120</span>, bias=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">  (fc2): Linear(in_features=<span class=\"number\">120</span>, out_features=<span class=\"number\">84</span>, bias=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">  (fc3): Linear(in_features=<span class=\"number\">84</span>, out_features=<span class=\"number\">10</span>, bias=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"number\">10</span></span><br><span class=\"line\">torch.Size([<span class=\"number\">6</span>, <span class=\"number\">1</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>])</span><br><span class=\"line\">torch.Size([<span class=\"number\">6</span>])</span><br><span class=\"line\">tensor([[ <span class=\"number\">0.1246</span>, <span class=\"number\">-0.0511</span>,  <span class=\"number\">0.0235</span>,  <span class=\"number\">0.1766</span>, <span class=\"number\">-0.0359</span>, <span class=\"number\">-0.0334</span>,  <span class=\"number\">0.1161</span>,  <span class=\"number\">0.0534</span>,</span><br><span class=\"line\">          <span class=\"number\">0.0282</span>, <span class=\"number\">-0.0202</span>]], grad_fn=&lt;ThAddmmBackward&gt;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"LOSS-FUNCTION\"><a href=\"#LOSS-FUNCTION\" class=\"headerlink\" title=\"LOSS FUNCTION\"></a>LOSS FUNCTION</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output = net(input)</span><br><span class=\"line\">target = torch.randn(<span class=\"number\">10</span>)  <span class=\"comment\"># a dummy target, for example</span></span><br><span class=\"line\">target = target.view(<span class=\"number\">1</span>, <span class=\"number\">-1</span>)  <span class=\"comment\"># make it the same shape as output</span></span><br><span class=\"line\">criterion = nn.MSELoss()</span><br><span class=\"line\"></span><br><span class=\"line\">loss = criterion(output, target)</span><br><span class=\"line\">print(loss)</span><br></pre></td></tr></table></figure>\n<h3 id=\"BACKPROP\"><a href=\"#BACKPROP\" class=\"headerlink\" title=\"BACKPROP\"></a>BACKPROP</h3><p>通过调用loss.backward()来进行反向误差传播，但在这之前要进行梯度清零，因为计算的梯度会累积起来，所以不清零的话下次减去的就是累积的梯度了，这是不对的！，举例：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.zero_grad()     <span class=\"comment\"># 梯度置为0</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'BP之前的梯度'</span>)</span><br><span class=\"line\">print(net.conv1.bias.grad)</span><br><span class=\"line\"></span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'BP之后的梯度'</span>)</span><br><span class=\"line\">print(net.conv1.bias.grad)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出</span></span><br><span class=\"line\">conv1.bias.grad before backward</span><br><span class=\"line\">tensor([<span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>])</span><br><span class=\"line\">conv1.bias.grad after backward</span><br><span class=\"line\">tensor([ <span class=\"number\">0.0181</span>, <span class=\"number\">-0.0048</span>, <span class=\"number\">-0.0229</span>, <span class=\"number\">-0.0138</span>, <span class=\"number\">-0.0088</span>, <span class=\"number\">-0.0107</span>])</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"UPDATE-THE-WEIGHTS\"><a href=\"#UPDATE-THE-WEIGHTS\" class=\"headerlink\" title=\"UPDATE THE WEIGHTS\"></a>UPDATE THE WEIGHTS</h3><p>参数更新公式：weight = weight - learning_rate * gradient</p>\n<p>这是最简单的更新形式，可以用简单的代码来实现：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">learning_rate = <span class=\"number\">0.01</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure></p>\n<p>但这种写法封装程度不高，而且在实现Adam, RMSProp等的时候就很复杂了，还好pytorch提供了封装好的库，在torch.optim里面<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># create your optimizer</span></span><br><span class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># in your training loop:</span></span><br><span class=\"line\">optimizer.zero_grad()   <span class=\"comment\"># zero the gradient buffers</span></span><br><span class=\"line\">output = net(input)</span><br><span class=\"line\">loss = criterion(output, target)</span><br><span class=\"line\">loss.backward()</span><br><span class=\"line\">optimizer.step()    <span class=\"comment\"># Does the update</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<h2 id=\"TRAINING-A-CLASSIFIER\"><a href=\"#TRAINING-A-CLASSIFIER\" class=\"headerlink\" title=\"TRAINING A CLASSIFIER\"></a>TRAINING A CLASSIFIER</h2><p>以 CIFAR10 dataset为例子，介绍了如何读取、显示、训练、测试等操作</p>\n<h3 id=\"Loading-and-normalizing-CIFAR10\"><a href=\"#Loading-and-normalizing-CIFAR10\" class=\"headerlink\" title=\"Loading and normalizing CIFAR10\"></a>Loading and normalizing CIFAR10</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对图像进行规范化操作</span></span><br><span class=\"line\">transform = transforms.Compose(</span><br><span class=\"line\">    [transforms.ToTensor(),</span><br><span class=\"line\">     transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>))])</span><br><span class=\"line\"></span><br><span class=\"line\">trainset = torchvision.datasets.CIFAR10(root=<span class=\"string\">'E:/database/cifar-10'</span>, train=<span class=\"keyword\">True</span>,</span><br><span class=\"line\">                                        download=<span class=\"keyword\">False</span>, transform=transform)</span><br><span class=\"line\">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class=\"number\">4</span>,</span><br><span class=\"line\">                                          shuffle=<span class=\"keyword\">True</span>, num_workers=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># testset = torchvision.datasets.CIFAR10(root='./data', train=False,</span></span><br><span class=\"line\"><span class=\"comment\">#                                        download=True, transform=transform)</span></span><br><span class=\"line\"><span class=\"comment\"># testloader = torch.utils.data.DataLoader(testset, batch_size=4,</span></span><br><span class=\"line\"><span class=\"comment\">#                                          shuffle=False, num_workers=2)</span></span><br><span class=\"line\"></span><br><span class=\"line\">classes = (<span class=\"string\">'plane'</span>, <span class=\"string\">'car'</span>, <span class=\"string\">'bird'</span>, <span class=\"string\">'cat'</span>,</span><br><span class=\"line\">           <span class=\"string\">'deer'</span>, <span class=\"string\">'dog'</span>, <span class=\"string\">'frog'</span>, <span class=\"string\">'horse'</span>, <span class=\"string\">'ship'</span>, <span class=\"string\">'truck'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">imshow</span><span class=\"params\">(img)</span>:</span></span><br><span class=\"line\">    img = img / <span class=\"number\">2</span> + <span class=\"number\">0.5</span>     <span class=\"comment\"># unnormalize</span></span><br><span class=\"line\">    npimg = img.numpy()</span><br><span class=\"line\">    plt.imshow(np.transpose(npimg, (<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>)))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\"># get some random training images</span></span><br><span class=\"line\">    dataiter = iter(trainloader)</span><br><span class=\"line\">    images, labels = dataiter.next()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># show images</span></span><br><span class=\"line\">    imshow(torchvision.utils.make_grid(images))</span><br><span class=\"line\">    <span class=\"comment\"># print labels</span></span><br><span class=\"line\">    print(<span class=\"string\">' '</span>.join(<span class=\"string\">'%5s'</span> % classes[labels[j]] <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>)))</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img-blog.csdnimg.cn/20181025220825123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=,size_27,color_FFFFFF,t_70\" style=\"zoom:80%\"> </p>\n<h3 id=\"train-and-test\"><a href=\"#train-and-test\" class=\"headerlink\" title=\"train and test\"></a>train and test</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义网络</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Net</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        super(Net, self).__init__()</span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.pool = nn.MaxPool2d(<span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">6</span>, <span class=\"number\">16</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>, <span class=\"number\">120</span>)</span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">120</span>, <span class=\"number\">84</span>)</span><br><span class=\"line\">        self.fc3 = nn.Linear(<span class=\"number\">84</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class=\"line\">        x = x.view(<span class=\"number\">-1</span>, <span class=\"number\">16</span> * <span class=\"number\">5</span> * <span class=\"number\">5</span>)</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = F.relu(self.fc2(x))</span><br><span class=\"line\">        x = self.fc3(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">net = Net()</span><br><span class=\"line\"></span><br><span class=\"line\">criterion = nn.CrossEntropyLoss()</span><br><span class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># train~~</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>):  <span class=\"comment\"># 1周期</span></span><br><span class=\"line\"></span><br><span class=\"line\">    running_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, data <span class=\"keyword\">in</span> enumerate(trainloader, <span class=\"number\">0</span>):</span><br><span class=\"line\">        <span class=\"comment\"># get the inputs</span></span><br><span class=\"line\">        inputs, labels = data</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 梯度清零</span></span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># forward + backward + optimize</span></span><br><span class=\"line\">        outputs = net(inputs)</span><br><span class=\"line\">        loss = criterion(outputs, labels)</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># verbose</span></span><br><span class=\"line\">        running_loss += loss.item()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">2000</span> == <span class=\"number\">1999</span>:    <span class=\"comment\"># print every 2000 mini-batches</span></span><br><span class=\"line\">            print(<span class=\"string\">'[%d, %5d] loss: %.3f'</span> %</span><br><span class=\"line\">                  (epoch + <span class=\"number\">1</span>, i + <span class=\"number\">1</span>, running_loss / <span class=\"number\">2000</span>))</span><br><span class=\"line\">            running_loss = <span class=\"number\">0.0</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'Finished Training'</span>)</span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">[<span class=\"number\">1</span>,  <span class=\"number\">2000</span>] loss: <span class=\"number\">2.191</span></span><br><span class=\"line\">[<span class=\"number\">1</span>,  <span class=\"number\">4000</span>] loss: <span class=\"number\">1.868</span></span><br><span class=\"line\">[<span class=\"number\">1</span>,  <span class=\"number\">6000</span>] loss: <span class=\"number\">1.684</span></span><br><span class=\"line\">[<span class=\"number\">1</span>,  <span class=\"number\">8000</span>] loss: <span class=\"number\">1.588</span></span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">10000</span>] loss: <span class=\"number\">1.524</span></span><br><span class=\"line\">[<span class=\"number\">1</span>, <span class=\"number\">12000</span>] loss: <span class=\"number\">1.487</span></span><br><span class=\"line\">Finished Training</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># test</span></span><br><span class=\"line\">correct = <span class=\"number\">0</span></span><br><span class=\"line\">total = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"comment\"># 因为是测试，所以不需要计算梯度</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    <span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:</span><br><span class=\"line\">        images, labels = data</span><br><span class=\"line\">        outputs = net(images)</span><br><span class=\"line\">        <span class=\"comment\"># 返回最大的数值和idx，这里我们只需要索引也就是predicted</span></span><br><span class=\"line\">        _, predicted = torch.max(outputs.data, <span class=\"number\">1</span>)</span><br><span class=\"line\">        total += labels.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        correct += (predicted == labels).sum().item()</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class=\"line\">    <span class=\"number\">100</span> * correct / total))</span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">Accuracy of the network on the <span class=\"number\">10000</span> test images: <span class=\"number\">47</span> %</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 每个类别的正确率</span></span><br><span class=\"line\">class_correct = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</span><br><span class=\"line\">class_total = list(<span class=\"number\">0.</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    <span class=\"keyword\">for</span> data <span class=\"keyword\">in</span> testloader:</span><br><span class=\"line\">        images, labels = data</span><br><span class=\"line\">        outputs = net(images)</span><br><span class=\"line\">        _, predicted = torch.max(outputs, <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 这里squeeze()是取出维度是1的维度，但我试了两个一维tensor(labels也是一维的)</span></span><br><span class=\"line\">        <span class=\"comment\"># 结果是加上squeeze()和不加没区别，这里不知道是什么意思</span></span><br><span class=\"line\">        c = (predicted == labels).squeeze()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">4</span>):</span><br><span class=\"line\">            label = labels[i]</span><br><span class=\"line\">            class_correct[label] += c[i].item()</span><br><span class=\"line\">            class_total[label] += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    print(<span class=\"string\">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class=\"line\">        classes[i], <span class=\"number\">100</span> * class_correct[i] / class_total[i]))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">Accuracy of plane : <span class=\"number\">43</span> %</span><br><span class=\"line\">Accuracy of   car : <span class=\"number\">48</span> %</span><br><span class=\"line\">Accuracy of  bird : <span class=\"number\">27</span> %</span><br><span class=\"line\">Accuracy of   cat :  <span class=\"number\">5</span> %</span><br><span class=\"line\">Accuracy of  deer : <span class=\"number\">24</span> %</span><br><span class=\"line\">Accuracy of   dog : <span class=\"number\">44</span> %</span><br><span class=\"line\">Accuracy of  frog : <span class=\"number\">73</span> %</span><br><span class=\"line\">Accuracy of horse : <span class=\"number\">67</span> %</span><br><span class=\"line\">Accuracy of  ship : <span class=\"number\">78</span> %</span><br><span class=\"line\">Accuracy of truck : <span class=\"number\">57</span> %</span><br></pre></td></tr></table></figure>\n<hr>\n<h2 id=\"TRAINING-ON-GPU\"><a href=\"#TRAINING-ON-GPU\" class=\"headerlink\" title=\"TRAINING ON GPU\"></a>TRAINING ON GPU</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">\"cuda:0\"</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">\"cpu\"</span>)</span><br><span class=\"line\"><span class=\"comment\"># 将网络送入gpu</span></span><br><span class=\"line\">net.to(device)</span><br><span class=\"line\"><span class=\"comment\"># 输入和label也必须送入gpu</span></span><br><span class=\"line\">inputs, labels = inputs.to(device), labels.to(device)</span><br></pre></td></tr></table></figure>\n<p>以上代码都运行了，文档说，没有感觉到gpu的加速效果是因为网络太小了，这是必然的；我运行的时候发现，不是加速不明显的问题了，而是比cpu也慢很多，好吧，也难怪，毕竟不停的将数据送入显存，然后把结果送回cpu输出，是会很耗费开销！！！以上初次了解Pytorch，确实和keras，TF不一样，用pytorch感觉就真的像是在写普通的python代码，而TF、keras真的是无时无刻不感受框架的存在和限制~~<br>这些只是简介，更多的函数用法等还需要去查看教程文档~~</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><p><a href=\"https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\" target=\"_blank\" rel=\"noopener\">https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html</a></p>\n</li>\n<li><p><a href=\"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#numpy-bridge\" target=\"_blank\" rel=\"noopener\">https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#numpy-bridge</a></p>\n</li>\n<li><p><a href=\"https://pytorch.org/docs/stable/torch.html?highlight=torch%20rand#torch.rand\" target=\"_blank\" rel=\"noopener\">https://pytorch.org/docs/stable/torch.html?highlight=torch%20rand#torch.rand</a></p>\n</li>\n</ul>\n","permalink":"https://www.ahbzzzh.cn/2018/11/21/pytorch入门文档/","categories":[{"name":"pytorch","slug":"pytorch","permalink":"https://www.ahbzzzh.cn/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://www.ahbzzzh.cn/tags/pytorch/"}]},{"title":"R-CNN","date":"2018-11-20T09:24:52.000Z","path":"2018/11/20/R-CNN/","content":"<p><img src=\"https://i.imgur.com/7uUW5DB.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<blockquote>\n<p>以往的目标检测算法不仅效果不好而且原理复杂，RCNN是一种简单并且可扩展的算法，在VOC2012数据集上mean average precision (mAP)提高了30%。RCNN主要使用了两个方法：(1) 使用CNN代替传统的特征提取方法来对候选区进行定位和分割 (2) 对于目标检测任务来说数据量是稀少的，所以采用了预训练CNN的策略。</p>\n</blockquote>\n<hr>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>&#160; &#160; &#160; &#160;特征问题，最初是利用SIFT和HOG并利用复杂的ensemble系统&#160;    的方法，后来由于基于生物上的视觉处理过程，由SIFT引入CNN，以及近些年CNN以成&#160; 为主流算法  （所以我得找个时间把上面得传统特征提取算法原理过一遍~）。由于近些年&#160; CNN在图像分类任务上  表现优异，本文作者就试图把图像分类任务扩展到目标探测上&#160; 来，实验表明利用CNN得优势相比传  统算法，在目标探测算法上取得了更好得效果。定&#160; 位问题和图像分类是不同的，其中，定位问题的   一种解决办法是被视为回归问题，然&#160; 而当时！这种方法取得效果并不好，另一种方法是利用滑窗的  思想；相反，RCNN原理是&#160; 对候选区进行识别。算法流程如下：<br><img src=\"https://img-blog.csdn.net/20181013225816655?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" style=\"zoom:80%\"><br>(1) 以整张图像作为输入<br>(2) 图去大约2000个候选区域<br>(3) 对每个候选区域利用CNN网络进行特征提取<br>(4) 利用特定类的线性SVM分类器对特征进行分类<br>&#160; &#160; &#160; &#160;需要注意的是，因为CNN需要相同维度的输入，所以要对候选区域进行reshape，但这样以来就会使目标对象变得扭曲进而丢失了其真实状态，对算法识别及泛化性方面有影响。在数据量稀少的情况下，为了防止模型过拟合会采用预训练的策略，目的是获得一个良好的初始化参数，预训练分为无监督和有监督，RCNN使用的是后者。</p>\n<hr>\n<h2 id=\"Object-detection-with-R-CNN\"><a href=\"#Object-detection-with-R-CNN\" class=\"headerlink\" title=\"Object detection with R-CNN\"></a>Object detection with R-CNN</h2><h3 id=\"Module-design\"><a href=\"#Module-design\" class=\"headerlink\" title=\"Module design\"></a>Module design</h3><p>&#160; &#160; &#160; &#160;Region proposals提取候选区的方法有很多：objectness, selective search, category-independent object proposals, constrained parametric min-cuts (CPMC)等，RCNN采用的selective search，作者解释说是为了更好的和以往的目标探测算法进行比较。Feature extraction对每个候选区域提取出4096维的特征向量，而输入则是227×227大小的RGB图像；在reshape之前，为了获得更多的区域上下文信息，对原始bbox进行扩充若干个像素。</p>\n<h3 id=\"Testtime-detection\"><a href=\"#Testtime-detection\" class=\"headerlink\" title=\"Testtime detection\"></a>Testtime detection</h3><p>&#160; &#160; &#160; &#160;测试阶段的重点是，在经过SVM之后，对所有的区域，针对独立的每个类应用greedy non-maximum suppression，NMS原理：<br><img src=\"https://img-blog.csdn.net/20181015173820730?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" style=\"zoom:80%\">                           </p>\n<h3 id=\"Training\"><a href=\"#Training\" class=\"headerlink\" title=\"Training\"></a>Training</h3><p>&#160; &#160; &#160; &#160;训练迭代过程中，batchsize为128(其中正样本为32，负样本为96)，为什么要有正负例呢？因为使用的是二分类器；正负样本的选取是依据IOU的，小于某个阈值则为负样本，选择不同的阈值对最终结果有着不同的影响。训练过程采用了hard negative- mining方法，因为样本中正样本所占比例较小，这样训练出来的分类效果不好，会出现很多False Positive(把负样本预测为正)的情况，hard-negative就把这些扔进网络再训练，从而加强分类器判别假阳性的能力。</p>\n<hr>\n<h2 id=\"消融研究-Ablation-studies\"><a href=\"#消融研究-Ablation-studies\" class=\"headerlink\" title=\"消融研究(Ablation studies)\"></a>消融研究(Ablation studies)</h2><p>ablation study就是为了研究模型中所提出的一些结构是否有效而设计的实验，也就是（控制变量法）<br><img src=\"https://img-blog.csdn.net/20181015215351217?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" style=\"zoom:80%\">   </p>\n<h2 id=\"Bounding-box-regression\"><a href=\"#Bounding-box-regression\" class=\"headerlink\" title=\"Bounding box regression\"></a>Bounding box regression</h2><p>基于pool5特征图利用线性回归模型对候选区域的box进行修正，实验表明此方法提高了3~4个百分点~</p>\n<hr>\n<h2 id=\"Appendix\"><a href=\"#Appendix\" class=\"headerlink\" title=\"Appendix\"></a>Appendix</h2><h3 id=\"Object-proposal-transformations\"><a href=\"#Object-proposal-transformations\" class=\"headerlink\" title=\"Object proposal transformations\"></a>Object proposal transformations</h3><p>A补充材料中介绍的是，作者尝试了两种不同reshape图像的方式，并分析了对最终结果的影响；第一种是各向同性的(isotropically)，第二种是各向异性的(anisotropically)，如图所示：<br><img src=\"https://img-blog.csdn.net/20181016090113208?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" style=\"zoom:80%\"><br>&#160; &#160; &#160; &#160;如图所示A代表这原始box边框，B属于各向同性的图像变换，C是各向异性的一个变体也属于这个范畴，D是各向异性的图像变换。所谓各项同性分为BC两种(“tightest square with context”和“tightest square without context”) ，根据英文就可以看出其意思，直白地说就是前者是先扩充图像上下文信息再reshap，后者是忽略周围地上下文信息使用灰色进行代替进而扩充成正方形；所谓各向异性就是直接对原图A进行reshape，从结果上看D图严重偏离了物体该有地自然形态，自然模型也就学习不到真实的东西。<br>&#160; &#160; &#160; &#160;对于每种变换，作者都进行了额外的上下文信息扩充，如图所示第一行的扩充像素为p=0，最后一行的扩充像素信息为p=16，p=16时会使得最终实验结果提升3~5个百分点。</p>\n<hr>\n<h3 id=\"Positive-vs-negative-examples-and-softmax\"><a href=\"#Positive-vs-negative-examples-and-softmax\" class=\"headerlink\" title=\"Positive vs. negative examples and softmax\"></a>Positive vs. negative examples and softmax</h3><blockquote>\n<p>为什么正负样本的定义对于softmax和svm是不同的呢？</p>\n</blockquote>\n<p>&#160; &#160; &#160; &#160;1、对于softmax正样本是IOU至少是0.5，其余的都是负样本；2、而SVM只使用真实原始box作为正样本，IOU小于0.3的才是负样本。作者解释，开始训练SVM时是基于Imagenet预训练模型的CNN特征的(那时候没有考虑使用微调)，比较下来2配置是很好的选择效果最佳；当开始使用微调时(使用softmax对候选区进行微调以适应样本)，使用2的配置发现效果很差。分析：怎么定义不是最重要的，重要的是微调需要一个大的样本来防止过拟合，而使用配置1正样本被扩展了30倍。这又产生了疑问，为什么要在微调之后再训练一个svm呢，不能直接用微调后的softmax输出吗？作者进行了实验，直接使用softmax会导致大约2歌百分点的map下降，原因可能是：1、微调的时候也就是配置1的样本定义方式在定位上是不准确的，因为IOU是0.5就可以算正样本了，这不科学；2、负样的采样方式是随机采样而不是使用hard negatives<br>&#160; &#160; &#160; &#160;总结，不使用SVM也可以获得接近但还有一点小差距的效果，如果有一些方法可以使微调阶段取得更好的效果，就可以放弃SVM阶段，这会简化和加快算法训练还不会损失算法性能。</p>\n<hr>\n<h3 id=\"Bounding-box-regression-1\"><a href=\"#Bounding-box-regression-1\" class=\"headerlink\" title=\"Bounding-box regression\"></a>Bounding-box regression</h3><p>&#160; &#160; &#160; &#160;假设有一组训练样本对：$\\left \\{ P^{i},G^{i} \\right \\}$，其中$P^{i}=P_{x}^{i},P_{y}^{i},P_{w}^{i},P_{z}^{i}$表示候选区域的中心坐标以及高度宽度，而$G^{i}=G_{x}^{i},G_{y}^{i},G_{w}^{i},G_{z}^{i}$则代表真实box边界框，我们的目的就是利用回归模型对候选区域box进行调整使其更复合真实bbox G。如图所示，其中$\\hat{G}$是我们预测的新的边界框，我们的学习目标就是新的边界框和真实框尽可能地接近：<br><img src=\"https://img-blog.csdn.net/20181016123235431?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" style=\"zoom:80%\"><br>&#160; &#160; &#160; &#160;对P框中心点进行位移，对长宽进行缩放，公式如下：</p>\n<script type=\"math/tex; mode=display\">\\hat{G}_{x}=P_{w}d_{x}(P)+P_{x}</script><script type=\"math/tex; mode=display\">\\hat{G}_{y}=P_{h}d_{y}(P)+P_{y}</script><script type=\"math/tex; mode=display\">\\hat{G}_{w}=P_{w}exp(d_{w}(P))</script><script type=\"math/tex; mode=display\">\\hat{G}_{h}=P_{h}exp(d_{h}(P))</script><p>&#160; &#160; &#160; &#160;其中d_(P)是作为一个线性函数，又是基于pool5特征的，则有：<script type=\"math/tex\">d_{x,y,w,h}(P)=w_{x,y,w,h}^{T}\\varnothing _{5}(P)</script>，我们通过优化一个最小二乘loss函数来得到最优参数W:<br><img src=\"https://img-blog.csdn.net/20181016131137279?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" style=\"zoom:70%\"><br>&#160; &#160; &#160; &#160;其中，t_表示原始边框P和真实边框G之间的映射，从上式可以看出，我们其实学习的是两种映射的近似，映射公式形式是一样的：</p>\n<script type=\"math/tex; mode=display\">t_{x}=(G_{x}-P_{x})/P_{w}</script><script type=\"math/tex; mode=display\">t_{y}=(G_{y}-P_{y})/P_{h}</script><script type=\"math/tex; mode=display\">t_{w}=log(G_{w}/G_{w})</script><script type=\"math/tex; mode=display\">t_{h}=log(G_{h}/G_{h})</script><p>&#160; &#160; &#160; &#160;作者还提到两个关于回归问题。1是loss函数的正则化参数设置问题；2就是当P相对于真实边界框G较远的时候，回归模型是不起作用的，在P和G  IOU较大时才起作用，原因是我们将映射看作是一个线性的，原因在这篇博客中解释的很好。</p>\n<hr>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ul>\n<li><a href=\"https://arxiv.org/abs/1311.2524v3\" target=\"_blank\" rel=\"noopener\">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>\n</ul>\n","permalink":"https://www.ahbzzzh.cn/2018/11/20/R-CNN/","categories":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/categories/目标检测/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/tags/目标检测/"},{"name":"论文","slug":"论文","permalink":"https://www.ahbzzzh.cn/tags/论文/"}]},{"title":"YOLO","date":"2018-11-16T12:39:17.000Z","path":"2018/11/16/YOLO/","content":"<p><img src=\"https://i.imgur.com/zKBlMhv.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<blockquote>\n<p>YOLO是一种真正端到端训练和预测的网络模型，它将目标探测作为一个回归问题来处理，直接从整张图片中预测bounding boxes和物体类别概率。YOLO最大的优势是在速度极快并且还实现了较高的检测性能，在显卡上运行可以达到45的帧率，fast版本可以达到155的帧率，与RCNN相比定位错误比较明显也是YOLO的主要劣势，但背景误检率比RCNN要低.</p>\n</blockquote>\n<hr>\n<h2 id=\"YOLO的优缺点：\"><a href=\"#YOLO的优缺点：\" class=\"headerlink\" title=\"YOLO的优缺点：\"></a>YOLO的优缺点：</h2><p>优点：<br>1、快、很快，不信可以去看<a href=\"http://pjreddie.com/yolo/\" target=\"_blank\" rel=\"noopener\">视频</a>.<br>2、YOLO是基于全图的目标检测，考虑了图像信息的上下文内容，而RCNN等则是先提取出候选区域再识别的(仅对特定区域进行处理)，这也是YOLO的背景误检率较低的原因.<br>3、泛化性很高，用常规图像训练，再对艺术照进行预测，也得到了很好的效果.<br>缺点是accuracy还不是最好的，其中主要定位错误，特别是小目标.</p>\n<hr>\n<h2 id=\"思想原理\"><a href=\"#思想原理\" class=\"headerlink\" title=\"思想原理\"></a>思想原理</h2><p>&#160; &#160; &#160; &#160;把图片分成S×S的格子，如果一个目标的中心落在了某个格子中，那么这个格子就负责探测这个目标；每个格子预测B个bbox和box对应的置信度confidence分数，置信度公式为：</p>\n<script type=\"math/tex; mode=display\">confidence=Pr(Object)\\ast IOU_{pred}^{truth}</script><p>&#160; &#160; &#160; &#160;如果没有目标在格子中，则置信度分数为0，否则置信度分数为IOU交并比.每个bounding box预测五个值，分别是x、y、w、h、置信度分数，(x，y)是与相对于格子的box的中心坐标，w、h则是相对于整张图像的box长宽。同时每个格子还预测C条件概率Pr(ClassijObject)，这里还要重点注意的是，无论每个格子预测多少个bounding box，规定都只预测一个类别。<br><img src=\"https://img-blog.csdnimg.cn/20181026231748893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=,size_27,color_FFFFFF,t_70\" style=\"zoom:80%\"><br>&#160; &#160; &#160; &#160;在测试时，每个box通过类别概率和box置信度相乘来得到特定类别置信分数：</p>\n<script type=\"math/tex; mode=display\">Pr(Class_{i}|Object)\\ast Pr(Object)\\ast IOU_{pred}^{truth}=Pr(Class_{i})\\ast IOU_{pred}^{truth}</script><p>&#160; &#160; &#160; &#160;这个分数衡量box中的类别概率以及预测框和目标的匹配程度。<br><img src=\"https://img-blog.csdnimg.cn/20181026231950699.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=,size_27,color_FFFFFF,t_70\" style=\"zoom:70%\"><br>&#160; &#160; &#160; &#160;24层卷积、2层全连接层；单位卷积用来减少网络规模；利用ImageNet(224×224)进行预训练；但探测的时候图像尺寸是448×448的。</p>\n<hr>\n<h2 id=\"loss函数\"><a href=\"#loss函数\" class=\"headerlink\" title=\"loss函数\"></a>loss函数</h2><p>&#160; &#160; &#160; &#160;使用的是平方和误差，要注意的是预测的bounding box五个值是要被归一化的，以免导致各个输出维度的取值范围差别很大，进而导致训练的时候，网络更关注数值大的维度。使用sum-squared error的优点是因为它是很好优化的，但缺点是它将位置预测和类别预测平等对待，这不合适，因为图像中大多数网格都是不包含目标的，这样优化下去会趋势这些格子的置信度分数为0，相对于包含目标的格子是不均衡的。<br>&#160; &#160; &#160; &#160;解决办法是赋予不同的权重：和，平方和误差对大框和小框的误差权衡是一样的，而我们的错误指标应该要体现出，大框的小偏差的重要性不如小框的小偏差的重要性(因为小框的一点偏差对结果影响大)。为了部分解决这个问题，我们直接预测边界框宽度和高度的平方根，而不是宽度和高度，为什么要这样就不解释了，画图即可。<br>&#160; &#160; &#160; &#160;YOLO为每个网格单元预测多个边界框。在训练时，每个目标我们只需要一个边界框预测器来负责。若某预测器的预测值与目标的实际值的IOU值最高，则这个预测器被指定为“负责”预测该目标。这导致边界框预测器的专业化。每个预测器可以更好地预测特定大小，方向角，或目标的类别，从而改善整体召回率。训练时还进行了数据增强、Non-maximal suppression等。<br><img src=\"https://img-blog.csdnimg.cn/20181026234551892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=,size_27,color_FFFFFF,t_70\" style=\"zoom:80%\">    </p>\n<hr>\n<h2 id=\"Limitations-of-YOLO\"><a href=\"#Limitations-of-YOLO\" class=\"headerlink\" title=\"Limitations of YOLO\"></a>Limitations of YOLO</h2><p>1、因为一个cell只能预测一个类，所以当小目标聚集的时候，就变得非常困难<br>2、预测的bbox毕竟是从数据集中学习到的，当预测没见过的长宽比例不同的物体时，就很难去泛化了<br>3、大小框误差不均衡问题，即使通过平方根技巧改变优化目标，仍然没改变错误定位的问题</p>\n<hr>\n<h2 id=\"速度对比\"><a href=\"#速度对比\" class=\"headerlink\" title=\"速度对比\"></a>速度对比</h2><p><img src=\"https://img-blog.csdnimg.cn/20181026235131207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=,size_27,color_FFFFFF,t_70\" style=\"zoom:80%\"> </p>\n<h2 id=\"VOC-2007-Error-Analysis\"><a href=\"#VOC-2007-Error-Analysis\" class=\"headerlink\" title=\"VOC 2007 Error Analysis\"></a>VOC 2007 Error Analysis</h2><p><img src=\"https://img-blog.csdnimg.cn/20181026235415986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6aDA5MDg=,size_27,color_FFFFFF,t_70\" style=\"zoom:80%\"><br>&#160; &#160; &#160; &#160;总结YOLO最牛批的地方在于，真正的端到端的目标检测算法，一种新的方式，速度极快！！！！但说实话，YOLO系列论文写的实在是太过于简单，其中详细原理还要去看代码!</p>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ul>\n<li><a href=\"https://arxiv.org/abs/1506.02640\" target=\"_blank\" rel=\"noopener\">You Only Look Once: Unified, Real-Time Object Detection</a></li>\n</ul>\n","permalink":"https://www.ahbzzzh.cn/2018/11/16/YOLO/","categories":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/categories/目标检测/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/tags/目标检测/"},{"name":"论文","slug":"论文","permalink":"https://www.ahbzzzh.cn/tags/论文/"}]},{"title":"YOLO V2","date":"2018-11-12T07:15:56.000Z","path":"2018/11/12/YOLO_V2/","content":"<p><img src=\"https://i.imgur.com/vpuEROf.png\" alt=\"\"><br><a id=\"more\"></a></p>\n<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>&#160; &#160; &#160; &#160;YOLO V2是YOLO的改进版本，当时在PASCAL vov和COCO数据集上都取得了最好的效果。因为之前YOLO非常不擅长小目标检测导致最终结果不是最好的，而本文采用了多尺度的训练方法，使YOLO V2能够具有尺度不变性，很好的平衡了模型准确率和速度因素。在VOC 2007上的数据结果：</p>\n<blockquote>\n<p>76.8mAP         67FPS<br>78.6mAP          40FPS<br>检测结果超过了当时的Faster RCNN和SSD，而且模型速度仍然是非常快的</p>\n</blockquote>\n<p>&#160; &#160; &#160; &#160;V2使用了联合训练的方法，使得模型可以同时在COCO检测数据集和ImageNet分类数据集上进行训练，通过这个方法使得模型可以检测到数据集中没有标注的检测数据。在ImageNet检测任务中取得了19.7mAP，注意这仅仅是在只训练过其中44个类别的情况下取得的。在COCO中没有的另外156个类别上，取得了16.0mAP。YOLO V2还能够预测9000个不同的目标类别并且保持实时性。</p>\n<h2 id=\"创新改进\"><a href=\"#创新改进\" class=\"headerlink\" title=\"创新改进\"></a>创新改进</h2><p>&#160; &#160; &#160; &#160;之前的YOLO有着大量的定位错误，为了提高召回率rell和定位能力同时保持类别预测率，主要使用改进了以下方法：YOLO V2没有采用更大规模的网络，而是简化了网络结构，只是采用了一些网络结构优化技巧，比如：</p>\n<h3 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h3><p>在YOLO基础上加上BN获得了2%mAP的提升，使模型更容易收敛，可以去除正则项、dropout等操作</p>\n<h3 id=\"High-Resolution-Classifie\"><a href=\"#High-Resolution-Classifie\" class=\"headerlink\" title=\"High Resolution Classifie\"></a>High Resolution Classifie</h3><p>&#160; &#160; &#160; &#160;YOLO分类微调是在ImageNet预训练模型上进行的，使用的输入分辨率为224×224，然后用448×448的输入进行目标检测任务，模型不能很好的适应。解决办法是，直接利用大的448×448输入进行分类模型的微调，这样就不用再重新适应不同的分辨率了，此改进获得了4%的mAP提升</p>\n<h3 id=\"Convolutional-With-Anchor-Boxes\"><a href=\"#Convolutional-With-Anchor-Boxes\" class=\"headerlink\" title=\"Convolutional With Anchor Boxes\"></a>Convolutional With Anchor Boxes</h3><p>&#160; &#160; &#160; &#160;YOLO是直接用基于卷积特征的全连接层来预测bboxes的坐标的，但全连接层会破坏特征图对应原图目标的位置信息，因为要reshape嘛，在可视化文章<a href=\"https://arxiv.org/abs/1512.04150\" target=\"_blank\" rel=\"noopener\">1</a> <a href=\"https://arxiv.org/abs/1610.02391\" target=\"_blank\" rel=\"noopener\">2</a>中也提到全连接层会破坏感兴趣区域的位置。而FRCNN中的RPN网络直接基于卷积特征图利用单位卷积获得输出，很好的保存了位置信息。<br>&#160; &#160; &#160; &#160;作者的做法是，移除全连层，移除一个池化层以获得一个大分辨率的特征图。通过调整网络使得输入尺寸为416，目的是为了获得一个奇数的中心点以预测在中心的目标，而不是使用四个临近位置来预测中心物体，这样更精确也更简单化，因为大的目标通常出现在图像中心。类别预测部分是和空间位置是分开的，和YOLO中cell的原理不同的是对每个Anchor都预测类别以及是目标的置信度。cell原理对于每张图片仅仅预测了98个Anchor，而V2可以预测上千个。实验表明，使用Anchor的方法使得mAP微弱下降，但召回率确提升了很多。</p>\n<h3 id=\"Dimension-Clusters\"><a href=\"#Dimension-Clusters\" class=\"headerlink\" title=\"Dimension Clusters\"></a>Dimension Clusters</h3><p>&#160; &#160; &#160; &#160;RPN中的anchor boxes的长宽比例是经过先验知识人为设定的，作者采用了一种更好的方法来选取boxes信息，即在训练数据上使用K-均值聚类的方法以自动的得出更优的先验。</p>\n","permalink":"https://www.ahbzzzh.cn/2018/11/12/YOLO_V2/","categories":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/categories/目标检测/"}],"tags":[{"name":"目标检测","slug":"目标检测","permalink":"https://www.ahbzzzh.cn/tags/目标检测/"},{"name":"论文","slug":"论文","permalink":"https://www.ahbzzzh.cn/tags/论文/"}]}]}